<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Guide · AdaptiveResonance.jl</title><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/favicon.ico" rel="icon" type="image/x-icon"/><link href="../../democards/gridtheme.css" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.png" alt="AdaptiveResonance.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">AdaptiveResonance.jl</a></span></div><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><span class="tocitem">Getting Started</span><ul><li><a class="tocitem" href="../../getting-started/whatisart/">Background</a></li><li><a class="tocitem" href="../../getting-started/basic-example/">Basic Example</a></li></ul></li><li><span class="tocitem">Tutorial</span><ul><li class="is-active"><a class="tocitem" href>Guide</a><ul class="internal"><li><a class="tocitem" href="#installation"><span>Installation</span></a></li><li><a class="tocitem" href="#art_modules"><span>ART Modules</span></a></li><li><a class="tocitem" href="#art_options"><span>ART Options</span></a></li><li><a class="tocitem" href="#art_vs_artmap"><span>ART vs. ARTMAP</span></a></li></ul></li><li><a class="tocitem" href="../../examples/">Examples</a></li><li><a class="tocitem" href="../modules/">Modules</a></li><li><a class="tocitem" href="../contributing/">Contributing</a></li><li><a class="tocitem" href="../full-index/">Index</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Tutorial</a></li><li class="is-active"><a href>Guide</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Guide</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/AP6YC/AdaptiveResonance.jl/blob/develop/docs/src/man/guide.md#L" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Package-Guide"><a class="docs-heading-anchor" href="#Package-Guide">Package Guide</a><a id="Package-Guide-1"></a><a class="docs-heading-anchor-permalink" href="#Package-Guide" title="Permalink"></a></h1><p>The <code>AdaptiveResonance.jl</code> package is built upon ART modules that contain all of the state information during training and inference. The ART modules are driven by options, which are themselves mutable keyword argument structs from the <a href="https://github.com/mauro3/Parameters.jl">Parameters.jl</a> package.</p><p>To work with <code>AdaptiveResonance.jl</code>, you should know:</p><ul><li><a href="#installation">How to install the package</a></li><li><a href="#art_modules">ART module basics</a></li><li><a href="#art_options">How to use ART module options</a></li><li><a href="#art_vs_artmap">ART vs. ARTMAP</a></li></ul><h2 id="installation"><a class="docs-heading-anchor" href="#installation">Installation</a><a id="installation-1"></a><a class="docs-heading-anchor-permalink" href="#installation" title="Permalink"></a></h2><p>The AdaptiveResonance package can be installed using the Julia package manager. From the Julia REPL, type <code>]</code> to enter the Pkg REPL mode and run</p><pre><code class="language-julia hljs">pkg&gt; add AdaptiveResonance</code></pre><p>Alternatively, it can be added to your environment in a script with</p><pre><code class="language-julia hljs">using Pkg
Pkg.add(&quot;AdaptiveResonance&quot;)</code></pre><p>If you wish to have the latest changes between releases, you can directly add the GitHub repo as a dependency with</p><pre><code class="language-julia hljs">pkg&gt; add https://github.com/AP6YC/AdaptiveResonance.jl</code></pre><h2 id="art_modules"><a class="docs-heading-anchor" href="#art_modules">ART Modules</a><a id="art_modules-1"></a><a class="docs-heading-anchor-permalink" href="#art_modules" title="Permalink"></a></h2><p>To work with ART modules, you should know:</p><ul><li><a href="#methods">Their basic methods</a></li><li><a href="#incremental_vs_batch">Incremental vs. batch modes</a></li><li><a href="#supervised_vs_unsupervised">Supervised vs. unsupervised learning modes</a></li><li><a href="#mismatch-bmu">Mismatch vs. Best-Matching-Unit</a></li></ul><h3 id="methods"><a class="docs-heading-anchor" href="#methods">Methods</a><a id="methods-1"></a><a class="docs-heading-anchor-permalink" href="#methods" title="Permalink"></a></h3><p>Every ART module is equipped with several constructors, a training function <code>train!</code>, and a classification/inference function <code>classify</code>. ART models are mutable structs, and they can be instantiated with</p><pre><code class="language-julia hljs">art = DDVFA()</code></pre><p>For more ways to customize instantiation, see the <a href="#art_options">ART options section</a>.</p><p>To train and test these models, you use the <code>train!</code> and <code>classify</code> functions upon the models. Because training changes the internal parameters of the ART models and classification does not, <code>train!</code> uses an exclamation point while <code>classify</code> does not, following Julia standard usage.</p><p>For example, we may load data of some sort and train/test like so:</p><pre><code class="language-julia hljs"># Load the data from some source with a train/test split
train_x, train_y, test_x, test_y = load_some_data()

# Instantiate an arbitrary ART module
art = DDVFA()

# Train the module on the training data, getting the prescribed cluster labels
y_hat_train = train!(art, train_x)

# Conduct inference
y_hat_test = classify(art, test_x)</code></pre><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>Because Julia arrays are column-major in memory, the <code>AdaptiveResonance.jl</code> package follows the Julia convention of assuming 2-D data arrays are in the shape of <code>(n_features, n_samples)</code>.</p></div></div><h3 id="incremental_vs_batch"><a class="docs-heading-anchor" href="#incremental_vs_batch">Incremental vs. Batch</a><a id="incremental_vs_batch-1"></a><a class="docs-heading-anchor-permalink" href="#incremental_vs_batch" title="Permalink"></a></h3><p>This training and testing may be done in either incremental or batch modes:</p><pre><code class="language-julia hljs"># Create a destination container for the incremental examples
n_train = length(train_y)
n_test = length(test_y)
y_hat_train_incremental = zeros(Integer, n_train)
y_hat_test_incremental = zeros(Integer, n_test)

# Loop over all training samples
for i = 1:n_train
    y_hat_train_incremental[i] = train!(art, train_x[:, i])
end

# loop over all testing samples
for i = 1:n_test
    y_hat_test_incremental[i] = classify(art, test_x[:, i])
end</code></pre><p>This is done through checking the dimensionality of the inputs. For example, if a matrix (i.e., 2-D array) is passed to the <code>train!</code> function, then the data is assumed to be <code>(n_features, n_samples)</code>, and the module is trained on all samples. However, if the data is a vector (i.e., 1-D array), then the vector is interpreted as a single sample.</p><p>When supervised (see <a href="#supervised_vs_unsupervised">supervised vs. unsupervised</a>), the dimensions of the labels must correspond to the dimensions of the data. For example, a 2-D matrix of the data must accompany a 1-D vector of labels, while a 1-D vector of a single data sample must accompany a single integer label.</p><p>Batch and incremental modes can be used interchangably after module instantiation.</p><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>The first time that an ART module is trained, it infers the data parameters (e.g., feature dimensions, feature ranges, etc.) to setup the internal data configuration. This happens automatically in batch mode, but it cannot happen if the module is only trained incrementally. If you know the dimensions and minimum/maximum values of the features and want to train incrementally, you can use the function <code>data_setup!</code> after module instantiation, which can be used a number of ways. If you have the batch data available, you can set up with</p><pre><code class="language-julia hljs"># Manually setup the data config with the data itself
data_setup!(art.config, data.train_x)</code></pre><p>If you do not have the batch data available, you can directly create a <code>DataConfig</code> with the minimums and maximums (inferring the number of features from the lengths of these vectors):</p><pre><code class="language-julia hljs"># Get the mins and maxes vectors with some method
mins, maxes = get_some_data_mins_maxes()

# Directly update the data config
art.config = DataConfig(mins, maxes)</code></pre><p>If all of the features share the same minimums and maximums, then you can use them as long as you specify the number of features:</p><pre><code class="language-julia hljs"># Get the global minimum, maximum, and feature dimension somehow
min, max, dim = get_some_data_characteristics()

# Directly update the data config with these global values
art.config = DataConfig(min, max, dim)</code></pre></div></div><h3 id="supervised_vs_unsupervised"><a class="docs-heading-anchor" href="#supervised_vs_unsupervised">Supervised vs. Unsupervised</a><a id="supervised_vs_unsupervised-1"></a><a class="docs-heading-anchor-permalink" href="#supervised_vs_unsupervised" title="Permalink"></a></h3><p>ARTMAP modules require a supervised label argument because their formulations typically map internal cluster categories to labels:</p><pre><code class="language-julia hljs"># Create an arbitrary ARTMAP module
artmap = DAM()

# Conduct supervised learning
y_hat_train = train!(artmap, train_x, train_y)

# Conduct inference
y_hat_test = classify(artmap, test_x)</code></pre><p>In the case of ARTMAP, the returned training labels <code>y_hat_train</code> will always match the training labels <code>train_y</code> by definition. In addition to the classification accuracy (ranging from 0 to 1), you can test that the training labels match with the function <code>performance</code>:</p><pre><code class="language-julia hljs"># Verify that the training labels match
perf_train = performance(y_hat_train, train_y)

# Get the classification accuracy
perf_test = performance(y_hat_test, test_y)</code></pre><p>However, many ART modules, though unsupervised by definition, can also be trained in a supervised way by naively mapping categories to labels (more in <a href="#art_vs_artmap">ART vs. ARTMAP</a>).</p><h3 id="mismatch-bmu"><a class="docs-heading-anchor" href="#mismatch-bmu">Mismatch vs. Best-Matching-Unit</a><a id="mismatch-bmu-1"></a><a class="docs-heading-anchor-permalink" href="#mismatch-bmu" title="Permalink"></a></h3><p>During inference, ART algorithms report the category that satisfies the match/vigilance criterion (see <a href="../../getting-started/whatisart/#Background">Background</a>). By default, in the case that no category satisfies this criterion the module reports a <em>mismatch</em> as -1. In modules that support it, a keyword argument <code>get_bmu</code> (default is <code>false</code>) can be used in the <code>classify</code> method to get the &quot;best-matching unit&quot;, which is the category that maximizes the activation. This can be interpreted as the &quot;next-best guess&quot; of the model in the case that the sample is sufficiently different from anything that the model has seen. For example,</p><pre><code class="language-julia hljs"># Conduct inference, getting the best-matching unit in case of complete mismatch
y_hat_bmu = classify(my_art, test_x, get_bmu=true)</code></pre><h2 id="art_options"><a class="docs-heading-anchor" href="#art_options">ART Options</a><a id="art_options-1"></a><a class="docs-heading-anchor-permalink" href="#art_options" title="Permalink"></a></h2><p>The AdaptiveResonance package is designed for maximum flexibility for scientific research, even though this may come at the cost of learning instability if misused. Because of the diversity of ART modules, the package is structured around instantiating separate modules and using them for training and inference. Due to this diversity, each module has its own options struct with keyword arguments. These options have default values driven by standards in their respective literatures, so the ART modules may be used immediately without any customization. Furthermore, these options are mutable, so they may be modified before module instantiation, before training, or even after training.</p><p>For example, you can get going with the default options by creating an ART module with the default constructor:</p><pre><code class="language-julia hljs">my_art = DDVFA()</code></pre><p>If you want to change the parameters before construction, you can create an options struct, modify it, then instantiate your ART module with it:</p><pre><code class="language-julia hljs">my_art_opts = opts_DDVFA()
my_art_opts.gamma = 3
my_art = DDVFA(my_art_opts)</code></pre><p>The options are objects from the <a href="https://github.com/mauro3/Parameters.jl">Parameters.jl</a> project, so they can be instantiated even with keyword arguments:</p><pre><code class="language-julia hljs">my_art_opts = opts_DDVFA(gamma = 3)</code></pre><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>As of version <code>0.3.6</code>, you can pass these keyword arguments directly to the ART model when constructing it with</p><pre><code class="language-julia hljs">my_art = DDVFA(gamma = 3)</code></pre></div></div><p>You can even modify the parameters on the fly after the ART module has been instantiated by directly modifying the options within the module:</p><pre><code class="language-julia hljs">my_art = DDVFA()
my_art.opts.gamma = 3</code></pre><p>Because of the <code>@assert</code> feature of the Parameters.jl package, each parameter is forced to lie within certain bounds by definition in the literature during options instantiation. However, it is possible to change these parameter values beyond their predefined bounds after instantiation.</p><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>You must be careful when changing option values during or after training, as it may result in some undefined behavior. Modify the ART module options after instantiation at your own risk and discretion.</p></div></div><p>Though most parameters differ between each ART and ARTMAP module, they all share some quality-of-life options and parameters shared by all ART algorithms:</p><ul><li><code>display::Bool</code>: a flag to display or suppress progress bars and logging messages during training and testing.</li><li><code>max_epochs::Integer</code>: the maximum number of epochs to train over the data, regardless if other stopping conditions have not been met yet.</li></ul><p>Otherwise, most ART and ARTMAP modules share the following nomenclature for algorithmic parameters:</p><ul><li><code>rho::Float</code>: ART vigilance parameter [0, 1].</li><li><code>alpha::Float</code>: Choice parameter &gt; 0.</li><li><code>beta::Float</code>: Learning parameter (0, 1].</li><li><code>epsilon::Float</code>: Match tracking parameter (0, 1).</li></ul><h2 id="art_vs_artmap"><a class="docs-heading-anchor" href="#art_vs_artmap">ART vs. ARTMAP</a><a id="art_vs_artmap-1"></a><a class="docs-heading-anchor-permalink" href="#art_vs_artmap" title="Permalink"></a></h2><p>ART modules are generally unsupervised in formulation, so they do not explicitly require supervisory labels to their training examples. However, many of these modules can be formulated in the simplified ARTMAP style whereby the ART B module has a vigilance parameter of 1, directly mapping the categories of the ART A module to any provided supervisory labels.</p><p>This is done in the training stage through the optional argument <code>y=...</code>:</p><pre><code class="language-julia hljs"># Create an arbitrary ART module
art = DDVFA()

# Naively prescribe supervised labels to cluster categories
y_hat_train = train!(art, train_x, y=train_y)</code></pre><p>This can also be done incrementally with the same function:</p><pre><code class="language-julia hljs"># Get the number of training samples and create a results container
n_train = length(train_y)
y_hat_train_incremental = zeros(Integer, n_train)

# Train incrementally over all training samples
for i = 1:n_train
    y_hat_train_incremental[i] = train!(art, train_x[:, i], y=train_y[i])
end</code></pre><p>Without provided labels, the ART modules behave as expected, incrementally creating categories when necessary during the training phase.</p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../../getting-started/basic-example/">« Basic Example</a><a class="docs-footer-nextpage" href="../../examples/">Examples »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.23 on <span class="colophon-date" title="Thursday 6 October 2022 23:01">Thursday 6 October 2022</span>. Using Julia version 1.8.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
