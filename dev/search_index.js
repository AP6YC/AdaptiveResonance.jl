var documenterSearchIndex = {"docs":
[{"location":"getting-started/basic-example/#Basic-Example","page":"Basic Example","title":"Basic Example","text":"","category":"section"},{"location":"getting-started/basic-example/","page":"Basic Example","title":"Basic Example","text":"In the example below, we create a dataset generated from two multivariate Gaussian distributions in two dimensions, showing how an ART module can be used in unsupervised or simple supervised modes alongside an ARTMAP module that is explicitly supervised-only.","category":"page"},{"location":"getting-started/basic-example/","page":"Basic Example","title":"Basic Example","text":"# Copyright © 2021 Alexander L. Hayes\n# MIT License\n\nusing AdaptiveResonance\nusing Distributions, Random\nusing MLDataUtils\nusing Plots\n\n\"\"\"\nDemonstrates Unsupervised DDVFA, Supervised DDVFA, and (Supervised) SFAM on a toy problem\nwith two multivariate Gaussians.\n\"\"\"\n\n# Setup two multivariate Gaussians and sampling 1000 points from each.\n\nrng = MersenneTwister(1234)\ndist1 = MvNormal([0.0, 6.0], [1.0 0.0; 0.0 1.0])\ndist2 = MvNormal([4.5, 6.0], [2.0 -1.5; -1.5 2.0])\n\nN_POINTS = 1000\n\nX = hcat(rand(rng, dist1, N_POINTS), rand(rng, dist2, N_POINTS))\ny = vcat(ones(Int64, N_POINTS), zeros(Int64, N_POINTS))\n\np1 = scatter(X[1,:], X[2,:], group=y, title=\"Original Data\")\n\n(X_train, y_train), (X_test, y_test) = stratifiedobs((X, y))\n\n# Standardize data types\nX_train = convert(Matrix{Float64}, X_train)\nX_test = convert(Matrix{Float64}, X_test)\ny_train = convert(Vector{Int}, y_train)\ny_test = convert(Vector{Int}, y_test)\n\n# Unsupervised DDVFA\nart = DDVFA()\ntrain!(art, X_train)\ny_hat_test = AdaptiveResonance.classify(art, X_test)\np2 = scatter(X_test[1,:], X_test[2,:], group=y_hat_test, title=\"Unsupervised DDVFA\")\n\n# Supervised DDVFA\nart = DDVFA()\ntrain!(art, X_train, y=y_train)\ny_hat_test = AdaptiveResonance.classify(art, X_test)\np3 = scatter(X_test[1,:], X_test[2,:], group=y_hat_test, title=\"Supervised DDVFA\", xlabel=\"Performance: \" * string(round(performance(y_hat_test, y_test); digits=3)))\n\n# Supervised SFAM\nart = SFAM()\ntrain!(art, X_train, y_train)\ny_hat_test = AdaptiveResonance.classify(art, X_test)\np4 = scatter(X_test[1,:], X_test[2,:], group=y_hat_test, title=\"Supervised SFAM\", xlabel=\"Performance: \" * string(round(performance(y_hat_test, y_test); digits=3)))\n\n# Performance Measure + display the plots\nplot(p1, p2, p3, p4, layout=(1, 4), legend = false, xtickfontsize=6, xguidefontsize=8, titlefont=font(8))","category":"page"},{"location":"man/contributing/#Contributing","page":"Contributing","title":"Contributing","text":"","category":"section"},{"location":"man/contributing/","page":"Contributing","title":"Contributing","text":"This page serves as the contribution guide for the AdaptiveResonance.jl package. From top to bottom, the ways of contributing are:","category":"page"},{"location":"man/contributing/","page":"Contributing","title":"Contributing","text":"GitHub Issues: how to raise an issue with the project.\nJulia Development: how to download and interact with the package.\nGitFlow: how to directly contribute code to the package in an organized way on GitHub.\nDevelopment Details: how the internals of the package are currently setup if you would like to directly contribute code.","category":"page"},{"location":"man/contributing/#Issues","page":"Contributing","title":"Issues","text":"","category":"section"},{"location":"man/contributing/","page":"Contributing","title":"Contributing","text":"The main point of contact is the GitHub issues page for the project. This is the easiest way to contribute to the project, as any issue you find or request you have will be addressed there by the authors of the package. Depending on the issue, the authors will collaborate with you, and after making changes they will link a pull request which addresses your concern or implements your proposed changes.","category":"page"},{"location":"man/contributing/#Julia-Development","page":"Contributing","title":"Julia Development","text":"","category":"section"},{"location":"man/contributing/","page":"Contributing","title":"Contributing","text":"As a Julia package, development follows the usual procedure:","category":"page"},{"location":"man/contributing/","page":"Contributing","title":"Contributing","text":"Clone the project from GitHub\nSwitch to or create the branch that you wish work on (see GitFlow).\nStart Julia at your development folder.\nInstantiate the package (i.e., download and install the package dependencies).","category":"page"},{"location":"man/contributing/","page":"Contributing","title":"Contributing","text":"For example, you can get the package and startup Julia with","category":"page"},{"location":"man/contributing/","page":"Contributing","title":"Contributing","text":"git clone git@github.com:AP6YC/AdaptiveResonance.jl.git\njulia --project=.","category":"page"},{"location":"man/contributing/","page":"Contributing","title":"Contributing","text":"note: Note\nIn Julia, you must activate your project in the current REPL to point to the location/scope of installed packages. The above immediately activates the project when starting up Julia, but you may also separately startup the julia and activate the package with the interactive package manager via the ] syntax:julia\njulia> ]\n(@v1.6) pkg> activate .\n(AdaptiveResonance) pkg>","category":"page"},{"location":"man/contributing/","page":"Contributing","title":"Contributing","text":"You may run the package's unit tests after the above setup in Julia with","category":"page"},{"location":"man/contributing/","page":"Contributing","title":"Contributing","text":"julia> using Pkg\njulia> Pkg.instantiate()\njulia> Pkg.test()","category":"page"},{"location":"man/contributing/","page":"Contributing","title":"Contributing","text":"or interactively though the Julia package manager with","category":"page"},{"location":"man/contributing/","page":"Contributing","title":"Contributing","text":"julia> ]\n(AdaptiveResonance) pkg> instantiate\n(AdaptiveResonance) pkg> test","category":"page"},{"location":"man/contributing/#GitFlow","page":"Contributing","title":"GitFlow","text":"","category":"section"},{"location":"man/contributing/","page":"Contributing","title":"Contributing","text":"As of verson 0.3.7, the AdaptiveResonance.jl package follows the GitFlow git working model. The original post by Vincent Driessen outlines this methodology quite well, while Atlassian has a good tutorial as well. In summary:","category":"page"},{"location":"man/contributing/","page":"Contributing","title":"Contributing","text":"Create a feature branch off of the develop branch with the name feature/<my-feature-name>.\nCommit your changes and push to this feature branch.\nWhen you are satisfied with your changes, initiate a GitHub pull request (PR) to merge the feature branch with develop.\nIf the unit tests pass, the feature branch will first be merged with develop and then be deleted.\nReleases will be periodically initiated from the develop branch and versioned onto the master branch.\nImmediate bug fixes circumvent this process through a hotfix branch off of master.","category":"page"},{"location":"man/contributing/#Development-Details","page":"Contributing","title":"Development Details","text":"","category":"section"},{"location":"man/contributing/#Documentation","page":"Contributing","title":"Documentation","text":"","category":"section"},{"location":"man/contributing/","page":"Contributing","title":"Contributing","text":"These docs are currently hosted as a static site on the GitHub pages platform. They are setup to be built and served in a separate branch gh-pages from the master/development branch of the project.","category":"page"},{"location":"man/contributing/#Package-Structure","page":"Contributing","title":"Package Structure","text":"","category":"section"},{"location":"man/contributing/","page":"Contributing","title":"Contributing","text":"The AdaptiveResonance.jl package has the following file structure:","category":"page"},{"location":"man/contributing/","page":"Contributing","title":"Contributing","text":"AdaptiveResonance\n├── .github/workflows       // GitHub: workflows for testing and documentation.\n├── data                    // Data: CI data location.\n├── docs                    // Docs: documentation for the module.\n│   └───src                 //      Documentation source files.\n├── examples                // Source: example usage scripts.\n├── src                     // Source: majority of source code.\n│   ├───ART                 //      ART-based unsupervised modules.\n│   └───ARTMAP              //      ARTMAP-based supervised modules.\n├── test                    // Test: Unit, integration, and environment tests.\n├── .appveyor               // Appveyor: Windows-specific coverage.\n├── .gitignore              // Git: .gitignore for the whole project.\n├── LICENSE                 // Doc: the license to the project.\n├── Project.toml            // Julia: the Pkg.jl dependencies of the project.\n└── README.md               // Doc: this document.","category":"page"},{"location":"man/contributing/","page":"Contributing","title":"Contributing","text":"ART and ARTMAP algorithms are put into their own files within the src/ART/ and src/ARTMAP/ directories, respectively. Both of these directories have an \"index\" file where each module is \"included\" (i.e., src/ART/ART.jl), which is in turn \"included\" in the package module file src/AdaptiveResonance.jl.","category":"page"},{"location":"man/contributing/","page":"Contributing","title":"Contributing","text":"Abstract types and common structures/methods are included at the top of the package module file. All public methods and structs (i.e., for the end user) are \"exported\" at the end of this file.","category":"page"},{"location":"man/contributing/#ART-Module-Workflow","page":"Contributing","title":"ART Module Workflow","text":"","category":"section"},{"location":"man/contributing/","page":"Contributing","title":"Contributing","text":"To write an ART module for this project, it will require the following:","category":"page"},{"location":"man/contributing/","page":"Contributing","title":"Contributing","text":"A train! and classify method (within the module).\nAn keyword-options struct using the Parameters.jl macro @with_kw with assertions to keep the parameters within correct ranges.\nThree constructors:\nA default constructor (i.e. DDVFA()).\nA keyword argument constructor (passing the kwargs to the options struct defined above).\nA constructor with the options struct passed itself.\nUse of common type aliases in method definitions.\nAn internal DataConfig for setting up the data configuration, especially with data_setup! (src/common.jl).\nAn update_iter evaluation for each iteration (src/common.jl).\nInclusion to the correct ART index file (i.e., src/ART/ART.jl).\nExports of the names for the options and module constructors in the module definition (src/AdaptiveResonance.jl).","category":"page"},{"location":"man/contributing/#DataConfig","page":"Contributing","title":"DataConfig","text":"","category":"section"},{"location":"man/contributing/","page":"Contributing","title":"Contributing","text":"The original implementation of ART1 uses binary vectors, which have guaranteed separation between distinct vectors. Real-valued ART modules, however, face the problem of permitting vectors to be arbitrarily close to one another. Therefore, nearly every real-valued ART module uses [0, 1] normalization and complement-coding. This is reflected in the DataConfig struct in the common file src/common.jl.","category":"page"},{"location":"man/contributing/#Type-Aliases","page":"Contributing","title":"Type Aliases","text":"","category":"section"},{"location":"man/contributing/","page":"Contributing","title":"Contributing","text":"In the pursuit of an architecture-agnostic implementation (i.e., support for both 32- and 64-bit systems), type aliases and other special Julia types are used in this project.","category":"page"},{"location":"man/contributing/","page":"Contributing","title":"Contributing","text":"This module borrows a convention from the StatsBase.jl package by defining a variety of aliases for numerical types used throughout the package to standardize usage. This has the benefits of readability and speed by explicitly These are defined in src/common.jl and are currently as follows:","category":"page"},{"location":"man/contributing/","page":"Contributing","title":"Contributing","text":"# Real-numbered aliases\nconst RealArray{T<:Real, N} = AbstractArray{T, N}\nconst RealVector{T<:Real} = AbstractArray{T, 1}\nconst RealMatrix{T<:Real} = AbstractArray{T, 2}\n\n# Integered aliases\nconst IntegerArray{T<:Integer, N} = AbstractArray{T, N}\nconst IntegerVector{T<:Integer} = AbstractArray{T, 1}\nconst IntegerMatrix{T<:Integer} = AbstractArray{T, 2}\n\n# Specifically floating-point aliases\nconst RealFP = Union{Float32, Float64}","category":"page"},{"location":"man/contributing/","page":"Contributing","title":"Contributing","text":"In this package, data samples are always Real-valued (with the notable exception of ART1), while class labels are integered. Furthermore, independent class labels are always Int because of the Julia native support for a given system's signed native integer type.","category":"page"},{"location":"man/contributing/","page":"Contributing","title":"Contributing","text":"This project does not currently test for the support of arbitrary precision arithmetic because learning algorithms in general do not have a significant need for precision beyond even 32-bit floats.","category":"page"},{"location":"man/contributing/#Authors","page":"Contributing","title":"Authors","text":"","category":"section"},{"location":"man/contributing/","page":"Contributing","title":"Contributing","text":"If you simply have suggestions for improvement, Sasha Petrenko (<sap625@mst.edu>) is the current developer and maintainer of the AdaptiveResonance.jl package, so please feel free to reach out with thoughts and questions.","category":"page"},{"location":"getting-started/whatisart/#Background","page":"Background","title":"Background","text":"","category":"section"},{"location":"getting-started/whatisart/","page":"Background","title":"Background","text":"This page provides a theoretical overview of Adaptive Resonance Theory and what this project aims to accomplish.","category":"page"},{"location":"getting-started/whatisart/#What-is-Adaptive-Resonance-Theory?","page":"Background","title":"What is Adaptive Resonance Theory?","text":"","category":"section"},{"location":"getting-started/whatisart/","page":"Background","title":"Background","text":"Adaptive Resonance Theory (commonly abbreviated to ART) is both a neurological theory and a family of neurogenitive neural network models for machine learning.","category":"page"},{"location":"getting-started/whatisart/","page":"Background","title":"Background","text":"ART began as a neurocognitive theory of how fields of cells can continuously learn stable representations, and it evolved into the basis for a myriad of practical machine learning algorithms. Pioneered by Stephen Grossberg and Gail Carpenter, the field has had contributions across many years and from many disciplines, resulting in a plethora of engineering applications and theoretical advancements that have enabled ART-based algorithms to compete with many other modern learning and clustering algorithms.","category":"page"},{"location":"getting-started/whatisart/","page":"Background","title":"Background","text":"Because of the high degree of interplay between the neurocognitive theory and the engineering models born of it, the term ART is frequently used to refer to both in the modern day (for better or for worse).","category":"page"},{"location":"getting-started/whatisart/","page":"Background","title":"Background","text":"Stephen Grossberg's has recently released a book summarizing the work of him, his wife Gail Carpenter, and his colleagues on Adaptive Resonance Theory in his book Conscious Brain, Resonant Mind.","category":"page"},{"location":"getting-started/whatisart/#ART-Basics","page":"Background","title":"ART Basics","text":"","category":"section"},{"location":"getting-started/whatisart/","page":"Background","title":"Background","text":"(Image: art)","category":"page"},{"location":"getting-started/whatisart/#ART-Dynamics","page":"Background","title":"ART Dynamics","text":"","category":"section"},{"location":"getting-started/whatisart/","page":"Background","title":"Background","text":"These are the basics of almost every ART model:","category":"page"},{"location":"getting-started/whatisart/","page":"Background","title":"Background","text":"ART models typically have two layers/fields denoted F1 and F2.\nThe F1 field is the feature representation field.  Most often, it is simply the input feature sample itself (after some necessary preprocessing).\nThe F2 field is the category representation field.  With some exceptions, each node in the F2 field generally represents its own category.  This is most easily understood as a weight vector representing a prototype for a class or centroid of a cluster.\nAn activation function is used to find the order of categories \"most activated\" for a given sample in F1.\nIn order of highest activation, a match function is used to compute the agreement between the sample and the categories.\nIf the match function evaluates to above a threshold value known as the vigilance parameter (rho), the weights may be updated according to a learning rule.\nIf there is complete mismatch across all categories, then a new categories is created according to some instantiation rule.","category":"page"},{"location":"getting-started/whatisart/#ART-Considerations","page":"Background","title":"ART Considerations","text":"","category":"section"},{"location":"getting-started/whatisart/","page":"Background","title":"Background","text":"In addition to the dynamics typical of an ART model, you must know:","category":"page"},{"location":"getting-started/whatisart/","page":"Background","title":"Background","text":"ART models are inherently designed for unsupervised learning (i.e., learning in the absense of supervisory labels for samples).  This is also known as clustering.\nART models are capable of supervised learning and reinforcement learning through some redesign and/or combination of ART models.  For example, ARTMAP models are combinations of two ART models in a special way, one learning feature-to-category mappings and another learning category-to-label mappingss.  ART modules are used for reinforcement learning by representing the mappings between state, value, and action spaces with ART dynamics.\nAlmost all ART models face the problem of the appropriate selection of the vigilance parameter, which may depend in its optimality according to the problem.\nBeing a class of neurogenitive neural network models, ART models gain the ability for theoretically infinite capacity along with the problem of \"category proliferation,\" which is the undesirable increase in the number of categories as the model continues to learn, leading to increasing computational time.  In contrast, while the evaluation time of a deep neural network is always exactly the same, there exist upper bounds in their representational capacity.\nNearly every ART model requires feature normalization (i.e., feature elements lying within 01) and a process known as complement coding where the feature vector is appended to its vector complement 1-barx. This is because real-numbered vectors can be arbitrarily close to one another, hindering learning performance, which requires a degree of contrast enhancement between samples to ensure their separation.","category":"page"},{"location":"getting-started/whatisart/","page":"Background","title":"Background","text":"To learn about their implementations, nearly every practical ART model is listed in a recent ART survey paper by Leonardo Enzo Brito da Silva.","category":"page"},{"location":"getting-started/whatisart/#History-and-Development","page":"Background","title":"History and Development","text":"","category":"section"},{"location":"getting-started/whatisart/","page":"Background","title":"Background","text":"At a high level, ART began with a neural network model known as the Grossberg Network named after Stephen Grossberg. This network treats the firing of neurons in frequency domain as basic shunting models, which are recurrently connected to increase their own activity while suppressing the activities of others nearby (i.e., on-center, off-surround). Using this shunting model, Grossberg shows that autonomous, associative learning can occur with what are known as instar networks.","category":"page"},{"location":"getting-started/whatisart/","page":"Background","title":"Background","text":"By representing categories as a field of instar networks, new categories could be optimally learned by the instantiation of new neurons. However, it was shown that the learning stability of Grossberg Networks degrades as the number of represented categories increases. Discoveries in the neurocognitive theory and breakthroughs in their implementation led to the introduction of a recurrent connections between the two fields of the network to stabilize the learning. These breakthroughs were based upon the discovery that autonomous learning depends on the interplay and agreement between perception and expectation, frequently referred to as bottom-up and top-down processes. Furthermore, it is resonance between these states in the frequency domain that gives rise to conscious experiences and that permit adaptive weights to change, leading to the phenomenon of learning. The theory has many explanatory consequences in psychology, such as why attention is required for learning, but its consequences in the engineering models are that it stabilizes learning in cooperative-competitive dynamics, such as interconnected fields of neurons, which are most often chaotic.","category":"page"},{"location":"getting-started/whatisart/","page":"Background","title":"Background","text":"Chapters 18 and 19 of the book by Neural Network Design by Hagan, Demuth, Beale, and De Jesus provide a good theoretical basis for learning how these network models were eventually implemented into the first binary-vector implementation of ART1.","category":"page"},{"location":"man/full-index/#main-index","page":"Index","title":"Index","text":"","category":"section"},{"location":"man/full-index/","page":"Index","title":"Index","text":"Pages = [\"lib/public.md\"]","category":"page"},{"location":"man/full-index/","page":"Index","title":"Index","text":"Modules = [AdaptiveResonance]\nPrivate = false","category":"page"},{"location":"man/full-index/#AdaptiveResonance.DAM","page":"Index","title":"AdaptiveResonance.DAM","text":"DAM <: ARTMAP\n\nDefault ARTMAP struct.\n\n\n\n\n\n","category":"type"},{"location":"man/full-index/#AdaptiveResonance.DAM-Tuple{opts_DAM}","page":"Index","title":"AdaptiveResonance.DAM","text":"DAM(opts)\n\nImplements a Default ARTMAP learner with specified options\n\nExamples\n\njulia> opts = opts_DAM()\njulia> DAM(opts)\nDAM\n    opts: opts_DAM\n    ...\n\n\n\n\n\n","category":"method"},{"location":"man/full-index/#AdaptiveResonance.DAM-Tuple{}","page":"Index","title":"AdaptiveResonance.DAM","text":"DAM(;kwargs...)\n\nImplements a Default ARTMAP learner with keyword arguments.\n\nExamples\n\njulia> DAM()\nDAM\n    opts: opts_DAM\n    ...\n\n\n\n\n\n","category":"method"},{"location":"man/full-index/#AdaptiveResonance.DDVFA","page":"Index","title":"AdaptiveResonance.DDVFA","text":"DDVFA <: ART\n\nDistributed Dual Vigilance Fuzzy ARTMAP module struct.\n\nExamples\n\njulia> DDVFA()\nDDVFA\n    opts: opts_DDVFA\n    subopts::opts_GNFA\n    ...\n\n\n\n\n\n","category":"type"},{"location":"man/full-index/#AdaptiveResonance.DDVFA-Tuple{opts_DDVFA}","page":"Index","title":"AdaptiveResonance.DDVFA","text":"DDVFA(opts::opts_DDVFA)\n\nImplements a DDVFA learner with specified options.\n\nExamples\n\njulia> my_opts = opts_DDVFA()\njulia> DDVFA(my_opts)\nDDVFA\n    opts: opts_DDVFA\n    subopts: opts_GNFA\n    ...\n\n\n\n\n\n","category":"method"},{"location":"man/full-index/#AdaptiveResonance.DDVFA-Tuple{}","page":"Index","title":"AdaptiveResonance.DDVFA","text":"DDVFA(;kwargs...)\n\nImplements a DDVFA learner with keyword arguments.\n\nExamples\n\njulia> DDVFA(rho=0.7)\nDDVFA\n    opts: opts_DDVFA\n    subopts: opts_GNFA\n    ...\n\n\n\n\n\n","category":"method"},{"location":"man/full-index/#AdaptiveResonance.DVFA","page":"Index","title":"AdaptiveResonance.DVFA","text":"DVFA <: ART\n\nDual Vigilance Fuzzy ARTMAP module struct.\n\nExamples\n\njulia> DVFA()\nDVFA\n    opts: opts_DVFA\n    ...\n\n\n\n\n\n","category":"type"},{"location":"man/full-index/#AdaptiveResonance.DVFA-Tuple{opts_DVFA}","page":"Index","title":"AdaptiveResonance.DVFA","text":"DVFA(opts::opts_DVFA)\n\nImplements a DVFA learner with specified options.\n\nExamples\n\njulia> my_opts = opts_DVFA()\njulia> DVFA(my_opts)\nDVFA\n    opts: opts_DVFA\n    ...\n\n\n\n\n\n","category":"method"},{"location":"man/full-index/#AdaptiveResonance.DVFA-Tuple{}","page":"Index","title":"AdaptiveResonance.DVFA","text":"DVFA(;kwargs...)\n\nImplements a DVFA learner with keyword arguments.\n\nExamples\n\njulia> DVFA(rho=0.7)\nDVFA\n    opts: opts_DDVFA\n    ...\n\n\n\n\n\n","category":"method"},{"location":"man/full-index/#AdaptiveResonance.DataConfig","page":"Index","title":"AdaptiveResonance.DataConfig","text":"DataConfig\n\nConatiner to standardize training/testing data configuration.\n\n\n\n\n\n","category":"type"},{"location":"man/full-index/#AdaptiveResonance.DataConfig-Tuple{AbstractArray{T,1} where T<:Real,AbstractArray{T,1} where T<:Real}","page":"Index","title":"AdaptiveResonance.DataConfig","text":"DataConfig(mins::RealVector, maxs::RealVector)\n\nConvenience constructor for DataConfig, requiring only mins and maxs of the features.\n\nThis constructor is used when the mins and maxs differ across features. The dimension is inferred by the length of the mins and maxs.\n\n\n\n\n\n","category":"method"},{"location":"man/full-index/#AdaptiveResonance.DataConfig-Tuple{AbstractArray{T,2} where T<:Real}","page":"Index","title":"AdaptiveResonance.DataConfig","text":"DataConfig(data::RealMatrix)\n\nConvenience constructor for DataConfig, requiring only the data matrix.\n\n\n\n\n\n","category":"method"},{"location":"man/full-index/#AdaptiveResonance.DataConfig-Tuple{Real,Real,Int64}","page":"Index","title":"AdaptiveResonance.DataConfig","text":"DataConfig(min::Real, max::Real, dim::Int)\n\nConvenience constructor for DataConfig, requiring only a global min, max, and dim.\n\nThis constructor is used in the case that the feature mins and maxs are all the same respectively.\n\n\n\n\n\n","category":"method"},{"location":"man/full-index/#AdaptiveResonance.DataConfig-Tuple{}","page":"Index","title":"AdaptiveResonance.DataConfig","text":"DataConfig()\n\nDefault constructor for a data configuration, not set up.\n\n\n\n\n\n","category":"method"},{"location":"man/full-index/#AdaptiveResonance.FAM","page":"Index","title":"AdaptiveResonance.FAM","text":"FAM <: ARTMAP\n\nFuzzy ARTMAP struct.\n\n\n\n\n\n","category":"type"},{"location":"man/full-index/#AdaptiveResonance.FAM-Tuple{opts_FAM}","page":"Index","title":"AdaptiveResonance.FAM","text":"FAM(opts)\n\nImplements a Fuzzy ARTMAP learner with specified options.\n\nExamples\n\njulia> opts = opts_FAM()\njulia> FAM(opts)\nFAM\n    opts: opts_FAM\n    ...\n\n\n\n\n\n","category":"method"},{"location":"man/full-index/#AdaptiveResonance.FAM-Tuple{}","page":"Index","title":"AdaptiveResonance.FAM","text":"FAM(;kwargs...)\n\nImplements a Fuzzy ARTMAP learner with keyword arguments.\n\nExamples\n\njulia> FAM(rho=0.7)\nFAM\n    opts: opts_FAM\n    ...\n\n\n\n\n\n","category":"method"},{"location":"man/full-index/#AdaptiveResonance.GNFA","page":"Index","title":"AdaptiveResonance.GNFA","text":"GNFA <: ART\n\nGamma-Normalized Fuzzy ART learner struct\n\nExamples\n\njulia> GNFA()\nGNFA\n    opts: opts_GNFA\n    ...\n\n\n\n\n\n","category":"type"},{"location":"man/full-index/#AdaptiveResonance.GNFA-Tuple{opts_GNFA,AbstractArray{T,N} where N where T<:Real}","page":"Index","title":"AdaptiveResonance.GNFA","text":"GNFA(opts::opts_GNFA, sample::RealArray)\n\nCreate and initialize a GNFA with a single sample in one step.\n\n\n\n\n\n","category":"method"},{"location":"man/full-index/#AdaptiveResonance.GNFA-Tuple{opts_GNFA}","page":"Index","title":"AdaptiveResonance.GNFA","text":"GNFA(opts::opts_GNFA)\n\nImplements a Gamma-Normalized Fuzzy ART learner with specified options.\n\nExamples\n\njulia> GNFA(opts)\nGNFA\n    opts: opts_GNFA\n    ...\n\n\n\n\n\n","category":"method"},{"location":"man/full-index/#AdaptiveResonance.GNFA-Tuple{}","page":"Index","title":"AdaptiveResonance.GNFA","text":"GNFA(;kwargs...)\n\nImplements a Gamma-Normalized Fuzzy ART learner with keyword arguments.\n\nExamples\n\njulia> GNFA(rho=0.7)\nGNFA\n    opts: opts_GNFA\n    ...\n\n\n\n\n\n","category":"method"},{"location":"man/full-index/#AdaptiveResonance.SFAM","page":"Index","title":"AdaptiveResonance.SFAM","text":"SFAM <: ARTMAP\n\nSimple Fuzzy ARTMAP struct.\n\n\n\n\n\n","category":"type"},{"location":"man/full-index/#AdaptiveResonance.SFAM-Tuple{opts_SFAM}","page":"Index","title":"AdaptiveResonance.SFAM","text":"SFAM(opts)\n\nImplements a Simple Fuzzy ARTMAP learner with specified options.\n\nExamples\n\njulia> opts = opts_SFAM()\njulia> SFAM(opts)\nSFAM\n    opts: opts_SFAM\n    ...\n\n\n\n\n\n","category":"method"},{"location":"man/full-index/#AdaptiveResonance.SFAM-Tuple{}","page":"Index","title":"AdaptiveResonance.SFAM","text":"SFAM(;kwargs...)\n\nImplements a Simple Fuzzy ARTMAP learner with keyword arguments.\n\nExamples\n\njulia> SFAM()\nSFAM\n    opts: opts_SFAM\n    ...\n\n\n\n\n\n","category":"method"},{"location":"man/full-index/#AdaptiveResonance.opts_DAM","page":"Index","title":"AdaptiveResonance.opts_DAM","text":"opts_DAM()\n\nImplements a Default ARTMAP learner's options.\n\nExamples\n\njulia> my_opts = opts_DAM()\n\n\n\n\n\n","category":"type"},{"location":"man/full-index/#AdaptiveResonance.opts_DDVFA","page":"Index","title":"AdaptiveResonance.opts_DDVFA","text":"opts_DDVFA()\n\nDistributed Dual Vigilance Fuzzy ART options struct.\n\nExamples\n\njulia> my_opts = opts_DDVFA()\n\n\n\n\n\n","category":"type"},{"location":"man/full-index/#AdaptiveResonance.opts_DVFA","page":"Index","title":"AdaptiveResonance.opts_DVFA","text":"opts_DVFA()\n\nDual Vigilance Fuzzy ART options struct.\n\nExamples\n\njulia> my_opts = opts_DVFA()\n\n\n\n\n\n","category":"type"},{"location":"man/full-index/#AdaptiveResonance.opts_FAM","page":"Index","title":"AdaptiveResonance.opts_FAM","text":"opts_FAM()\n\nImplements a Fuzzy ARTMAP learner's options.\n\nExamples\n\njulia> my_opts = opts_FAM()\n\n\n\n\n\n","category":"type"},{"location":"man/full-index/#AdaptiveResonance.opts_GNFA","page":"Index","title":"AdaptiveResonance.opts_GNFA","text":"opts_GNFA()\n\nGamma-Normalized Fuzzy ART options struct.\n\nExamples\n\njulia> opts_GNFA()\nInitialized GNFA\n\n\n\n\n\n","category":"type"},{"location":"man/full-index/#AdaptiveResonance.opts_SFAM","page":"Index","title":"AdaptiveResonance.opts_SFAM","text":"opts_SFAM()\n\nImplements a Simple Fuzzy ARTMAP learner's options.\n\nExamples\n\njulia> my_opts = opts_SFAM()\n\n\n\n\n\n","category":"type"},{"location":"man/full-index/#AdaptiveResonance.artscene_filter-Union{Tuple{Array{T,3}}, Tuple{T}} where T<:Real","page":"Index","title":"AdaptiveResonance.artscene_filter","text":"artscene_filter(raw_image::Array{T, 3} ;  distributed::Bool=true) where {T<:Real}\n\nProcess the full artscene filter toolchain on an image.\n\n\n\n\n\n","category":"method"},{"location":"man/full-index/#AdaptiveResonance.classify-Tuple{DAM,AbstractArray{T,2} where T<:Real}","page":"Index","title":"AdaptiveResonance.classify","text":"classify(art::DAM, x::RealMatrix ; preprocessed::Bool=false)\n\nCategorize data 'x' using a trained Default ARTMAP module 'art'.\n\nExamples\n\njulia> x, y = load_data()\njulia> x_test, y_test = load_test_data()\njulia> art = DAM()\nDAM\n    opts: opts_DAM\n    ...\njulia> train!(art, x, y)\njulia> classify(art, x_test)\n\n\n\n\n\n","category":"method"},{"location":"man/full-index/#AdaptiveResonance.classify-Tuple{DDVFA,AbstractArray{T,N} where N where T<:Real}","page":"Index","title":"AdaptiveResonance.classify","text":"classify(art::DDVFA, x::RealArray ; preprocessed::Bool=false, get_bmu::Bool=false)\n\nPredict categories of 'x' using the DDVFA model.\n\nReturns predicted categories 'y_hat.'\n\nExamples\n\njulia> my_DDVFA = DDVFA()\nDDVFA\n    opts: opts_DDVFA\n    ...\njulia> x, y = load_data()\njulia> train!(my_DDVFA, x)\njulia> y_hat = classify(my_DDVFA, y)\n\n\n\n\n\n","category":"method"},{"location":"man/full-index/#AdaptiveResonance.classify-Tuple{DVFA,AbstractArray{T,N} where N where T<:Real}","page":"Index","title":"AdaptiveResonance.classify","text":"classify(art::DVFA, x::RealArray)\n\nPredict categories of 'x' using the DVFA model.\n\nReturns predicted categories 'y_hat'\n\nExamples\n\njulia> my_DVFA = DVFA()\nDVFA\n    opts: opts_DVFA\n    ...\njulia> x, y = load_data()\njulia> train!(my_DVFA, x)\njulia> y_hat = classify(my_DVFA, y)\n\n\n\n\n\n","category":"method"},{"location":"man/full-index/#AdaptiveResonance.classify-Tuple{GNFA,AbstractArray{T,N} where N where T<:Real}","page":"Index","title":"AdaptiveResonance.classify","text":"classify(art::GNFA, x::RealArray)\n\nPredict categories of 'x' using the GNFA model.\n\nReturns predicted categories 'y_hat'\n\nExamples\n\njulia> my_GNFA = GNFA()\nGNFA\n    opts: opts_GNFA\n    ...\njulia> x, y = load_data()\njulia> train!(my_GNFA, x)\njulia> y_hat = classify(my_GNFA, y)\n\n\n\n\n\n","category":"method"},{"location":"man/full-index/#AdaptiveResonance.classify-Tuple{SFAM,AbstractArray{T,N} where N where T<:Real}","page":"Index","title":"AdaptiveResonance.classify","text":"classify(art::SFAM, x::RealArray ; preprocessed::Bool=false)\n\nCategorize data 'x' using a trained Simple Fuzzy ARTMAP module 'art'.\n\nExamples\n\njulia> x, y = load_data()\njulia> x_test, y_test = load_test_data()\njulia> art = SFAM()\nSFAM\n    opts: opts_SFAM\n    ...\njulia> train!(art, x, y)\njulia> classify(art, x_test)\n\n\n\n\n\n","category":"method"},{"location":"man/full-index/#AdaptiveResonance.color_to_gray-Tuple{AbstractArray{T,N} where N where T<:Real}","page":"Index","title":"AdaptiveResonance.color_to_gray","text":"color_to_gray(image::RealArray)\n\nARTSCENE Stage 1: Color-to-gray image transformation.\n\n\n\n\n\n","category":"method"},{"location":"man/full-index/#AdaptiveResonance.complement_code-Tuple{AbstractArray{T,N} where N where T<:Real}","page":"Index","title":"AdaptiveResonance.complement_code","text":"complement_code(data::RealArray ; config::DataConfig=DataConfig())\n\nNormalize the data x to [0, 1] and returns the augmented vector [x, 1 - x].\n\n\n\n\n\n","category":"method"},{"location":"man/full-index/#AdaptiveResonance.contrast_insensitive_oriented_filtering-Tuple{AbstractArray{T,N} where N where T<:Real}","page":"Index","title":"AdaptiveResonance.contrast_insensitive_oriented_filtering","text":"contrast_insensitive_oriented_filtering(y::RealArray)\n\nARTSCENE Stage 4: Contrast-insensitive oriented filtering.\n\n\n\n\n\n","category":"method"},{"location":"man/full-index/#AdaptiveResonance.contrast_normalization-Tuple{AbstractArray{T,N} where N where T<:Real}","page":"Index","title":"AdaptiveResonance.contrast_normalization","text":"contrast_normalization(image::RealArray ; distributed::Bool=true)\n\nARTSCENE Stage 2: Constrast normalization.\n\n\n\n\n\n","category":"method"},{"location":"man/full-index/#AdaptiveResonance.contrast_sensitive_oriented_filtering-Tuple{AbstractArray{T,N} where N where T<:Real,AbstractArray{T,N} where N where T<:Real}","page":"Index","title":"AdaptiveResonance.contrast_sensitive_oriented_filtering","text":"contrast_sensitive_oriented_filtering(image::RealArray, x::RealArray ; distributed::Bool=true)\n\nARTSCENE Stage 3: Contrast-sensitive oriented filtering.\n\n\n\n\n\n","category":"method"},{"location":"man/full-index/#AdaptiveResonance.data_setup!-Tuple{DataConfig,AbstractArray{T,2} where T<:Real}","page":"Index","title":"AdaptiveResonance.data_setup!","text":"data_setup!(config::DataConfig, data::RealMatrix)\n\nSets up the data config for the ART module before training.\n\n\n\n\n\n","category":"method"},{"location":"man/full-index/#AdaptiveResonance.get_data_characteristics-Tuple{AbstractArray{T,N} where N where T<:Real}","page":"Index","title":"AdaptiveResonance.get_data_characteristics","text":"get_data_characteristics(data::RealArray ; config::DataConfig=DataConfig())\n\nGet the characteristics of the data, taking account if a data config is passed.\n\nIf no DataConfig is passed, then the data characteristics come from the array itself. Otherwise, use the config for the statistics of the data and the data array for the number of samples.\n\n\n\n\n\n","category":"method"},{"location":"man/full-index/#AdaptiveResonance.get_data_shape-Tuple{AbstractArray{T,N} where N where T<:Real}","page":"Index","title":"AdaptiveResonance.get_data_shape","text":"get_data_shape(data::RealArray)\n\nReturns the correct feature dimension and number of samples.\n\n\n\n\n\n","category":"method"},{"location":"man/full-index/#AdaptiveResonance.get_n_samples-Tuple{AbstractArray{T,N} where N where T<:Real}","page":"Index","title":"AdaptiveResonance.get_n_samples","text":"get_n_samples(data::RealArray)\n\nReturns the number of samples, accounting for 1-D and 2-D arrays.\n\n\n\n\n\n","category":"method"},{"location":"man/full-index/#AdaptiveResonance.linear_normalization-Tuple{AbstractArray{T,N} where N where T<:Real}","page":"Index","title":"AdaptiveResonance.linear_normalization","text":"linear_normalization(data::RealArray ; config::DataConfig=DataConfig())\n\nNormalize the data to the range [0, 1] along each feature.\n\n\n\n\n\n","category":"method"},{"location":"man/full-index/#AdaptiveResonance.orientation_competition-Tuple{AbstractArray{T,N} where N where T<:Real}","page":"Index","title":"AdaptiveResonance.orientation_competition","text":"orientation_competition(z::RealArray)\n\nARTSCENE Stage 5: Orientation competition at the same position.\n\n\n\n\n\n","category":"method"},{"location":"man/full-index/#AdaptiveResonance.patch_orientation_color-Tuple{AbstractArray{T,N} where N where T<:Real,AbstractArray{T,N} where N where T<:Real}","page":"Index","title":"AdaptiveResonance.patch_orientation_color","text":"patch_orientation_color(z::RealArray, image::RealArray)\n\nARTSCENE Stage 6: Create patch feature vectors.\n\n\n\n\n\n","category":"method"},{"location":"man/full-index/#AdaptiveResonance.performance-Tuple{AbstractArray{T,1} where T<:Integer,AbstractArray{T,1} where T<:Integer}","page":"Index","title":"AdaptiveResonance.performance","text":"performance(y_hat::IntegerVector, y::IntegerVector)\n\nReturns the categorization performance of y_hat against y.\n\n\n\n\n\n","category":"method"},{"location":"man/full-index/#AdaptiveResonance.train!-Tuple{DAM,AbstractArray{T,2} where T<:Real,AbstractArray{T,1} where T<:Integer}","page":"Index","title":"AdaptiveResonance.train!","text":"train!(art::DAM, x::RealMatrix, y::IntegerVector ; preprocessed::Bool=false)\n\nTrains a Default ARTMAP learner in a supervised manner.\n\nExamples\n\njulia> x, y = load_data()\njulia> art = DAM()\nDAM\n    opts: opts_DAM\n    ...\njulia> train!(art, x, y)\n\n\n\n\n\n","category":"method"},{"location":"man/full-index/#AdaptiveResonance.train!-Tuple{DDVFA,AbstractArray{T,N} where N where T<:Real}","page":"Index","title":"AdaptiveResonance.train!","text":"train!(art::DDVFA, x::RealArray ; y::IntegerVector=Vector{Int}(), preprocessed::Bool=false)\n\nTrain the DDVFA model on the data.\n\n\n\n\n\n","category":"method"},{"location":"man/full-index/#AdaptiveResonance.train!-Tuple{DVFA,AbstractArray{T,N} where N where T<:Real}","page":"Index","title":"AdaptiveResonance.train!","text":"train!(art::DVFA, x::RealArray ; y::IntegerVector = Vector{Int}(), preprocessed::Bool=false)\n\nTrain the DVFA module on x with optional custom category labels y.\n\nArguments\n\nart::DVFA: the dual-vigilance fuzzy art module to train.\nx::RealArray: the data to train on, interpreted as a single sample if x is a vector.\ny::IntegerVector=[]: optional custom labels to assign to the categories. If empty, ordinary incremental labels are prescribed.\n\n\n\n\n\n","category":"method"},{"location":"man/full-index/#AdaptiveResonance.train!-Tuple{GNFA,AbstractArray{T,N} where N where T<:Real}","page":"Index","title":"AdaptiveResonance.train!","text":"train!(art::GNFA, x::RealArray ; y::IntegerVector=[])\n\nTrains a GNFA learner with dataset 'x' and optional labels 'y'\n\nExamples\n\njulia> my_GNFA = GNFA()\nGNFA\n    opts: opts_GNFA\n    ...\njulia> x = load_data()\njulia> train!(my_GNFA, x)\n\n\n\n\n\n","category":"method"},{"location":"man/full-index/#AdaptiveResonance.train!-Tuple{SFAM,AbstractArray{T,N} where N where T<:Real,AbstractArray{T,N} where N where T<:Real}","page":"Index","title":"AdaptiveResonance.train!","text":"train!(art::SFAM, x::RealArray, y::RealArray ; preprocessed::Bool=false)\n\nTrains a Simple Fuzzy ARTMAP learner in a supervised manner.\n\nExamples\n\njulia> x, y = load_data()\njulia> art = SFAM()\nSFAM\n    opts: opts_SFAM\n    ...\njulia> train!(art, x, y)\n\n\n\n\n\n","category":"method"},{"location":"man/guide/#Package-Guide","page":"Guide","title":"Package Guide","text":"","category":"section"},{"location":"man/guide/","page":"Guide","title":"Guide","text":"The AdaptiveResonance.jl package is built upon ART modules that contain all of the state information during training and inference. The ART modules are driven by options, which are themselves mutable keyword argument structs from the Parameters.jl package.","category":"page"},{"location":"man/guide/","page":"Guide","title":"Guide","text":"To work with AdaptiveResonance.jl, you should know:","category":"page"},{"location":"man/guide/","page":"Guide","title":"Guide","text":"How to install the package\nART module basics\nHow to use ART module options\nART vs. ARTMAP","category":"page"},{"location":"man/guide/#installation","page":"Guide","title":"Installation","text":"","category":"section"},{"location":"man/guide/","page":"Guide","title":"Guide","text":"The AdaptiveResonance package can be installed using the Julia package manager. From the Julia REPL, type ] to enter the Pkg REPL mode and run","category":"page"},{"location":"man/guide/","page":"Guide","title":"Guide","text":"pkg> add AdaptiveResonance","category":"page"},{"location":"man/guide/","page":"Guide","title":"Guide","text":"Alternatively, it can be added to your environment in a script with","category":"page"},{"location":"man/guide/","page":"Guide","title":"Guide","text":"using Pkg\nPkg.add(\"AdaptiveResonance\")","category":"page"},{"location":"man/guide/","page":"Guide","title":"Guide","text":"If you wish to have the latest changes between releases, you can directly add the GitHub repo as a dependency with","category":"page"},{"location":"man/guide/","page":"Guide","title":"Guide","text":"pkg> add https://github.com/AP6YC/AdaptiveResonance.jl","category":"page"},{"location":"man/guide/#art_modules","page":"Guide","title":"ART Modules","text":"","category":"section"},{"location":"man/guide/","page":"Guide","title":"Guide","text":"To work with ART modules, you should know:","category":"page"},{"location":"man/guide/","page":"Guide","title":"Guide","text":"Their basic methods\nIncremental vs. batch modes\nSupervised vs. unsupervised learning modes","category":"page"},{"location":"man/guide/#methods","page":"Guide","title":"Methods","text":"","category":"section"},{"location":"man/guide/","page":"Guide","title":"Guide","text":"Every ART module is equipped with several constructors, a training function train!, and a classification/inference function classify. ART models are mutable structs, and they can be instantiated with","category":"page"},{"location":"man/guide/","page":"Guide","title":"Guide","text":"art = DDVFA()","category":"page"},{"location":"man/guide/","page":"Guide","title":"Guide","text":"For more ways to customize instantiation, see the ART options section.","category":"page"},{"location":"man/guide/","page":"Guide","title":"Guide","text":"To train and test these models, you use the train! and classify functions upon the models. Because training changes the internal parameters of the ART models and classification does not, train! uses an exclamation point while classify does not, following Julia standard usage.","category":"page"},{"location":"man/guide/","page":"Guide","title":"Guide","text":"For example, we may load data of some sort and train/test like so:","category":"page"},{"location":"man/guide/","page":"Guide","title":"Guide","text":"# Load the data from some source with a train/test split\ntrain_x, train_y, test_x, test_y = load_some_data()\n\n# Instantiate an arbitrary ART module\nart = DDVFA()\n\n# Train the module on the training data, getting the prescribed cluster labels\ny_hat_train = train!(art, train_x)\n\n# Conduct inference\ny_hat_test = classify(art, test_x)","category":"page"},{"location":"man/guide/","page":"Guide","title":"Guide","text":"note: Note\nBecause Julia arrays are column-major in memory, the AdaptiveResonance.jl package follows the Julia convention of assuming 2-D data arrays are in the shape of (n_features, n_samples).","category":"page"},{"location":"man/guide/#incremental_vs_batch","page":"Guide","title":"Incremental vs. Batch","text":"","category":"section"},{"location":"man/guide/","page":"Guide","title":"Guide","text":"This training and testing may be done in either incremental or batch modes:","category":"page"},{"location":"man/guide/","page":"Guide","title":"Guide","text":"# Create a destination container for the incremental examples\nn_train = length(train_y)\nn_test = length(test_y)\ny_hat_train_incremental = zeros(Integer, n_train)\ny_hat_test_incremental = zeros(Integer, n_test)\n\n# Loop over all training samples\nfor i = 1:n_train\n    y_hat_train_incremental[i] = train!(art, train_x[:, i])\nend\n\n# loop over all testing samples\nfor i = 1:n_test\n    y_hat_test_incremental[i] = classify(art, test_x[:, i])\nend","category":"page"},{"location":"man/guide/","page":"Guide","title":"Guide","text":"This is done through checking the dimensionality of the inputs. For example, if a matrix (i.e., 2-D array) is passed to the train! function, then the data is assumed to be (n_features, n_samples), and the module is trained on all samples. However, if the data is a vector (i.e., 1-D array), then the vector is interpreted as a single sample.","category":"page"},{"location":"man/guide/","page":"Guide","title":"Guide","text":"When supervised (see supervised vs. unsupervised), the dimensions of the labels must correspond to the dimensions of the data. For example, a 2-D matrix of the data must accompany a 1-D vector of labels, while a 1-D vector of a single data sample must accompany a single integer label.","category":"page"},{"location":"man/guide/","page":"Guide","title":"Guide","text":"Batch and incremental modes can be used interchangably after module instantiation.","category":"page"},{"location":"man/guide/","page":"Guide","title":"Guide","text":"note: Note\nThe first time that an ART module is trained, it infers the data parameters (e.g., feature dimensions, feature ranges, etc.) to setup the internal data configuration. This happens automatically in batch mode, but it cannot happen if the module is only trained incrementally. If you know the dimensions and minimum/maximum values of the features and want to train incrementally, you can use the function data_setup! after module instantiation, which can be used a number of ways. If you have the batch data available, you can set up with# Manually setup the data config with the data itself\ndata_setup!(art.config, data.train_x)If you do not have the batch data available, you can directly create a DataConfig with the minimums and maximums (inferring the number of features from the lengths of these vectors):# Get the mins and maxes vectors with some method\nmins, maxes = get_some_data_mins_maxes()\n\n# Directly update the data config\nart.config = DataConfig(mins, maxes)If all of the features share the same minimums and maximums, then you can use them as long as you specify the number of features:# Get the global minimum, maximum, and feature dimension somehow\nmin, max, dim = get_some_data_characteristics()\n\n# Directly update the data config with these global values\nart.config = DataConfig(min, max, dim)","category":"page"},{"location":"man/guide/#supervised_vs_unsupervised","page":"Guide","title":"Supervised vs. Unsupervised","text":"","category":"section"},{"location":"man/guide/","page":"Guide","title":"Guide","text":"ARTMAP modules require a supervised label argument because their formulations typically map internal cluster categories to labels:","category":"page"},{"location":"man/guide/","page":"Guide","title":"Guide","text":"# Create an arbitrary ARTMAP module\nartmap = DAM()\n\n# Conduct supervised learning\ny_hat_train = train!(artmap, train_x, train_y)\n\n# Conduct inference\ny_hat_test = classify(artmap, test_x)","category":"page"},{"location":"man/guide/","page":"Guide","title":"Guide","text":"In the case of ARTMAP, the returned training labels y_hat_train will always match the training labels train_y by definition. In addition to the classification accuracy (ranging from 0 to 1), you can test that the training labels match with the function performance:","category":"page"},{"location":"man/guide/","page":"Guide","title":"Guide","text":"# Verify that the training labels match\nperf_train = performance(y_hat_train, train_y)\n\n# Get the classification accuracy\nperf_test = performance(y_hat_test, test_y)","category":"page"},{"location":"man/guide/","page":"Guide","title":"Guide","text":"However, many ART modules, though unsupervised by definition, can also be trained in a supervised way by naively mapping categories to labels (more in ART vs. ARTMAP).","category":"page"},{"location":"man/guide/#art_options","page":"Guide","title":"ART Options","text":"","category":"section"},{"location":"man/guide/","page":"Guide","title":"Guide","text":"The AdaptiveResonance package is designed for maximum flexibility for scientific research, even though this may come at the cost of learning instability if misused. Because of the diversity of ART modules, the package is structured around instantiating separate modules and using them for training and inference. Due to this diversity, each module has its own options struct with keyword arguments. These options have default values driven by standards in their respective literatures, so the ART modules may be used immediately without any customization. Furthermore, these options are mutable, so they may be modified before module instantiation, before training, or even after training.","category":"page"},{"location":"man/guide/","page":"Guide","title":"Guide","text":"For example, you can get going with the default options by creating an ART module with the default constructor:","category":"page"},{"location":"man/guide/","page":"Guide","title":"Guide","text":"my_art = DDVFA()","category":"page"},{"location":"man/guide/","page":"Guide","title":"Guide","text":"If you want to change the parameters before construction, you can create an options struct, modify it, then instantiate your ART module with it:","category":"page"},{"location":"man/guide/","page":"Guide","title":"Guide","text":"my_art_opts = opts_DDVFA()\nmy_art_opts.gamma = 3\nmy_art = DDVFA(my_art_opts)","category":"page"},{"location":"man/guide/","page":"Guide","title":"Guide","text":"The options are objects from the Parameters.jl project, so they can be instantiated even with keyword arguments:","category":"page"},{"location":"man/guide/","page":"Guide","title":"Guide","text":"my_art_opts = opts_DDVFA(gamma = 3)","category":"page"},{"location":"man/guide/","page":"Guide","title":"Guide","text":"note: Note\nAs of version 0.3.6, you can pass these keyword arguments directly to the ART model when constructing it withmy_art = DDVFA(gamma = 3)","category":"page"},{"location":"man/guide/","page":"Guide","title":"Guide","text":"You can even modify the parameters on the fly after the ART module has been instantiated by directly modifying the options within the module:","category":"page"},{"location":"man/guide/","page":"Guide","title":"Guide","text":"my_art = DDVFA()\nmy_art.opts.gamma = 3","category":"page"},{"location":"man/guide/","page":"Guide","title":"Guide","text":"Because of the @assert feature of the Parameters.jl package, each parameter is forced to lie within certain bounds by definition in the literature during options instantiation. However, it is possible to change these parameter values beyond their predefined bounds after instantiation.","category":"page"},{"location":"man/guide/","page":"Guide","title":"Guide","text":"note: Note\nYou must be careful when changing option values during or after training, as it may result in some undefined behavior. Modify the ART module options after instantiation at your own risk and discretion.","category":"page"},{"location":"man/guide/","page":"Guide","title":"Guide","text":"Though most parameters differ between each ART and ARTMAP module, they all share some quality-of-life options and parameters shared by all ART algorithms:","category":"page"},{"location":"man/guide/","page":"Guide","title":"Guide","text":"display::Bool: a flag to display or suppress progress bars and logging messages during training and testing.\nmax_epochs::Integer: the maximum number of epochs to train over the data, regardless if other stopping conditions have not been met yet.","category":"page"},{"location":"man/guide/","page":"Guide","title":"Guide","text":"Otherwise, most ART and ARTMAP modules share the following nomenclature for algorithmic parameters:","category":"page"},{"location":"man/guide/","page":"Guide","title":"Guide","text":"rho::Float: ART vigilance parameter [0, 1].\nalpha::Float: Choice parameter > 0.\nbeta::Float: Learning parameter (0, 1].\nepsilon::Float: Match tracking parameter (0, 1).","category":"page"},{"location":"man/guide/#art_vs_artmap","page":"Guide","title":"ART vs. ARTMAP","text":"","category":"section"},{"location":"man/guide/","page":"Guide","title":"Guide","text":"ART modules are generally unsupervised in formulation, so they do not explicitly require supervisory labels to their training examples. However, many of these modules can be formulated in the simplified ARTMAP style whereby the ART B module has a vigilance parameter of 1, directly mapping the categories of the ART A module to any provided supervisory labels.","category":"page"},{"location":"man/guide/","page":"Guide","title":"Guide","text":"This is done in the training stage through the optional argument y=...:","category":"page"},{"location":"man/guide/","page":"Guide","title":"Guide","text":"# Create an arbitrary ART module\nart = DDVFA()\n\n# Naively prescribe supervised labels to cluster categories\ny_hat_train = train!(art, train_x, y=train_y)","category":"page"},{"location":"man/guide/","page":"Guide","title":"Guide","text":"This can also be done incrementally with the same function:","category":"page"},{"location":"man/guide/","page":"Guide","title":"Guide","text":"# Get the number of training samples and create a results container\nn_train = length(train_y)\ny_hat_train_incremental = zeros(Integer, n_train)\n\n# Train incrementally over all training samples\nfor i = 1:n_train\n    y_hat_train_incremental[i] = train!(art, train_x[:, i], y=train_y[i])\nend","category":"page"},{"location":"man/guide/","page":"Guide","title":"Guide","text":"Without provided labels, the ART modules behave as expected, incrementally creating categories when necessary during the training phase.","category":"page"},{"location":"man/examples/#Examples","page":"Examples","title":"Examples","text":"","category":"section"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"There are examples for every structure in the package within the package's examples/ folder. The code for several of these examples is provided here.","category":"page"},{"location":"man/examples/#ART","page":"Examples","title":"ART","text":"","category":"section"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"All ART modules learn in an unsupervised (i.e. clustering) mode by default, but they all can accept labels in the simplified ARTMAP fashion (see the Package Guide).","category":"page"},{"location":"man/examples/#DDVFA-Unsupervised","page":"Examples","title":"DDVFA Unsupervised","text":"","category":"section"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"DDVFA is an unsupervised clustering algorithm by definition, so it can be used to cluster a set of samples all at once in batch mode.","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"# Load the data from some source with a train/test split\ntrain_x, train_y, test_x, test_y = load_some_data()\n\n# Instantiate a DDVFA module\nart = DDVFA()\n\n# Train the module on the training data, getting the prescribed cluster labels\ny_hat_train = train!(art, train_x)\n\n# Conduct inference\ny_hat_test = classify(art, test_x)","category":"page"},{"location":"man/examples/#DDVFA-Supervised","page":"Examples","title":"DDVFA Supervised","text":"","category":"section"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"ART modules such as DDVFA can also be used in simple supervised mode where provided labels are used in place of internal incremental labels for the clusters, providing a method of assessing the clustering performance when labels are available.","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"# Load the data from some source with a train/test split\ntrain_x, train_y, test_x, test_y = load_some_data()\n\n# Instantiate a DDVFA module\nart = DDVFA()\n\n# Train the module on the training data, getting the prescribed cluster labels\ny_hat_train = train!(art, train_x, y=train_y)\n\n# Conduct inference\ny_hat_test = classify(art, test_x)\n\n# Verify that the training labels match\nperf_train = performance(y_hat_train, train_y)\n\n# Get the classification accuracy\nperf_test = performance(y_hat_test, test_y)","category":"page"},{"location":"man/examples/#Incremental-DDVFA-With-Custom-Options-and-Data-Configuration","page":"Examples","title":"Incremental DDVFA With Custom Options and Data Configuration","text":"","category":"section"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"Even more advanced, DDVFA can be run incrementally (i.e. with one sample at a time) with custom algorithmic options and a predetermined data configuration. It is necessary to provide a data configuration if the model is not pretrained because the model has no knowledge of the boundaries and dimensionality of the data, which are necessary in the complement coding step.","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"# Load the data from some source with a train/test split\ntrain_x, train_y, test_x, test_y = load_some_data()\n\n# Create custom DDVFA options\nopts = opts_DDVFA(gamma=3)\n\n# Instantiate a DDVFA module with the specified options\nart = DDVFA(opts)\n\n# Change the options after instantiation for fun\nart.opts.rho_lb = 0.5\n\n# Customize the data configuration\n# Assume that we have prior knowledge that the features lie within [0, 1]\n# and that they have dimension 10\nart.config = DataConfig(0, 1, 10)\n\n# Create data containers for label results\nn_train = length(train_x)\nn_test = length(test_x)\ny_hat_train = zeros(Integer, n_train)\ny_hat_test = zeros(Integer, n_test)\n\n# Train the module on the training data incrementally, getting the prescribed cluster labels\nfor i = 1:n_train\n    sample = train_x[:, i]\n    label = train_y[i]\n    y_hat_train[i] = train!(art, sample, y=label)\nend\n\n# Conduct inference incrementally\nfor i = 1:n_test\n    sample = test_x[:, i]\n    y_hat_test[i] = classify(art, sample)\nend\n\n# Verify that the training labels match\nperf_train = performance(y_hat_train, train_y)\n\n# Get the classification accuracy\nperf_test = performance(y_hat_test, test_y)","category":"page"},{"location":"man/examples/#ARTMAP","page":"Examples","title":"ARTMAP","text":"","category":"section"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"ARTMAP modules are supervised by definition, so the require supervised labels in the training stage.","category":"page"},{"location":"man/examples/#SFAM","page":"Examples","title":"SFAM","text":"","category":"section"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"A Simplified FuzzyARTMAP can be used to learn supervised mappings on features directly and in batch mode.","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"# Load the data from some source with a train/test split\ntrain_x, train_y, test_x, test_y = load_some_data()\n\n# Create a Simplified Fuzzy ARTMAP module\nart = SFAM()\n\n# Train in batch\ny_hat_train = train!(art, train_x, train_y)\n\n# Test in batch\ny_hat_test = classify(art, test_x)\n\n# Verify that the training labels match\nperf_train = performance(y_hat_train, train_y)\n\n# Calculate testing performance\nperf_test = performance(y_hat_test, test_y)","category":"page"},{"location":"man/examples/#Incremental-SFAM-With-Custom-Options-and-Data-Configuration","page":"Examples","title":"Incremental SFAM With Custom Options and Data Configuration","text":"","category":"section"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"A simplified FuzzyARTMAP can also be run iteratively, assuming that we know the statistics of the features ahead of time and reflect that in the module's config with a DataConfig object.","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"# Load the data from some source with a train/test split\ntrain_x, train_y, test_x, test_y = load_some_data()\n\n# Create custom SFAM options\nopts = opts_SFAM(rho=0.5)\n\n# Instantiate a SFAM module with the specified options\nart = SFAM(opts)\n\n# Change the options after instantiation for fun\nart.opts.epsilon = 1e-2\n\n# Customize the data configuration\n# Assume that we have prior knowledge that the features lie within [0, 1]\n# and that they have dimension 10\nart.config = DataConfig(0, 1, 10)\n\n# Create data containers for label results\nn_train = length(train_x)\nn_test = length(test_x)\ny_hat_train = zeros(Integer, n_train)\ny_hat_test = zeros(Integer, n_test)\n\n# Train the module on the training data incrementally, getting the prescribed cluster labels\nfor i = 1:n_train\n    sample = train_x[:, i]\n    label = train_y[i]\n    y_hat_train[i] = train!(art, sample, label)\nend\n\n# Conduct inference incrementally\nfor i = 1:n_test\n    sample = test_x[:, i]\n    y_hat_test[i] = classify(art, sample)\nend\n\n# Verify that the training labels match\nperf_train = performance(y_hat_train, train_y)\n\n# Get the classification accuracy\nperf_test = performance(y_hat_test, test_y)","category":"page"},{"location":"","page":"Home","title":"Home","text":"(Image: header)","category":"page"},{"location":"","page":"Home","title":"Home","text":"","category":"page"},{"location":"#AdaptiveResonance.jl","page":"Home","title":"AdaptiveResonance.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"These pages serve as the official documentation for the AdaptiveResonance.jl Julia package.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Adaptive Resonance Theory (ART) began as a neurocognitive theory of how fields of cells can continuously learn stable representations, and it evolved into the basis for a myriad of practical machine learning algorithms. Pioneered by Stephen Grossberg and Gail Carpenter, the field has had contributions across many years and from many disciplines, resulting in a plethora of engineering applications and theoretical advancements that have enabled ART-based algorithms to compete with many other modern learning and clustering algorithms.","category":"page"},{"location":"","page":"Home","title":"Home","text":"The purpose of this package is to provide a home for the development and use of these ART-based machine learning algorithms.","category":"page"},{"location":"","page":"Home","title":"Home","text":"See the Index for the complete list of documented functions and types.","category":"page"},{"location":"#Manual-Outline","page":"Home","title":"Manual Outline","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"This documentation is split into the following sections:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Pages = [\n    \"man/guide.md\",\n    \"man/examples.md\",\n    \"man/contributing.md\",\n    \"man/full-index.md\",\n]\nDepth = 1","category":"page"},{"location":"","page":"Home","title":"Home","text":"The Package Guide provides a tutorial to the full usage of the package, while Examples gives sample workflows using a variety of ART modules.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Instructions on how to contribute to the package are found in Contributing, and docstrings for every element of the package is listed in the Index.","category":"page"}]
}
