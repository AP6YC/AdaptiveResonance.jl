var documenterSearchIndex = {"docs":
[{"location":"getting-started/basic-example/#Basic-Example","page":"Basic Example","title":"Basic Example","text":"","category":"section"},{"location":"getting-started/basic-example/","page":"Basic Example","title":"Basic Example","text":"This page demonstrates a full basic example of an AdaptiveResonance.jl workflow. In the example below, we create a dataset generated from two multivariate Gaussian distributions in two dimensions, showing how an ART module can be used in unsupervised or simple supervised modes alongside an ARTMAP module that is explicitly supervised-only.","category":"page"},{"location":"getting-started/basic-example/","page":"Basic Example","title":"Basic Example","text":"For more examples that you can run yourself in Julia notebooks, see the Examples page.","category":"page"},{"location":"getting-started/basic-example/","page":"Basic Example","title":"Basic Example","text":"# Copyright © 2021 Alexander L. Hayes\n# MIT License\n\nusing AdaptiveResonance\nusing Distributions, Random\nusing MLDataUtils\nusing Plots\npyplot()\n\n\"\"\"\nDemonstrates Unsupervised DDVFA, Supervised DDVFA, and (Supervised) SFAM on a toy problem\nwith two multivariate Gaussians.\n\"\"\"\n\n# Setup two multivariate Gaussians and sampling 1000 points from each.\n\nrng = MersenneTwister(1234)\ndist1 = MvNormal([0.0, 6.0], [1.0 0.0; 0.0 1.0])\ndist2 = MvNormal([4.5, 6.0], [2.0 -1.5; -1.5 2.0])\n\nN_POINTS = 1000\n\nX = hcat(rand(rng, dist1, N_POINTS), rand(rng, dist2, N_POINTS))\ny = vcat(ones(Int64, N_POINTS), zeros(Int64, N_POINTS))\n\np1 = scatter(X[1,:], X[2,:], group=y, title=\"Original Data\")\n\n(X_train, y_train), (X_test, y_test) = stratifiedobs((X, y))\n\n# Standardize data types\nX_train = convert(Matrix{Float64}, X_train)\nX_test = convert(Matrix{Float64}, X_test)\ny_train = convert(Vector{Int}, y_train)\ny_test = convert(Vector{Int}, y_test)\n\n# Unsupervised DDVFA\nart = DDVFA()\ntrain!(art, X_train)\ny_hat_test = AdaptiveResonance.classify(art, X_test)\np2 = scatter(X_test[1,:], X_test[2,:], group=y_hat_test, title=\"Unsupervised DDVFA\")\n\n# Supervised DDVFA\nart = DDVFA()\ntrain!(art, X_train, y=y_train)\ny_hat_test = AdaptiveResonance.classify(art, X_test)\np3 = scatter(X_test[1,:], X_test[2,:], group=y_hat_test, title=\"Supervised DDVFA\", xlabel=\"Performance: \" * string(round(performance(y_hat_test, y_test); digits=3)))\n\n# Supervised SFAM\nart = SFAM()\ntrain!(art, X_train, y_train)\ny_hat_test = AdaptiveResonance.classify(art, X_test)\np4 = scatter(X_test[1,:], X_test[2,:], group=y_hat_test, title=\"Supervised SFAM\", xlabel=\"Performance: \" * string(round(performance(y_hat_test, y_test); digits=3)))\n\n# Performance Measure + display the plots\nplot(p1, p2, p3, p4, layout=(1, 4), legend = false, xtickfontsize=6, xguidefontsize=8, titlefont=font(8))","category":"page"},{"location":"man/contributing/#Contributing","page":"Contributing","title":"Contributing","text":"","category":"section"},{"location":"man/contributing/","page":"Contributing","title":"Contributing","text":"This page serves as the contribution guide for the AdaptiveResonance.jl package. From top to bottom, the ways of contributing are:","category":"page"},{"location":"man/contributing/","page":"Contributing","title":"Contributing","text":"GitHub Issues: how to raise an issue with the project.\nJulia Development: how to download and interact with the package.\nGitFlow: how to directly contribute code to the package in an organized way on GitHub.\nDevelopment Details: how the internals of the package are currently setup if you would like to directly contribute code.","category":"page"},{"location":"man/contributing/#Issues","page":"Contributing","title":"Issues","text":"","category":"section"},{"location":"man/contributing/","page":"Contributing","title":"Contributing","text":"The main point of contact is the GitHub issues page for the project. This is the easiest way to contribute to the project, as any issue you find or request you have will be addressed there by the authors of the package. Depending on the issue, the authors will collaborate with you, and after making changes they will link a pull request which addresses your concern or implements your proposed changes.","category":"page"},{"location":"man/contributing/#Julia-Development","page":"Contributing","title":"Julia Development","text":"","category":"section"},{"location":"man/contributing/","page":"Contributing","title":"Contributing","text":"As a Julia package, development follows the usual procedure:","category":"page"},{"location":"man/contributing/","page":"Contributing","title":"Contributing","text":"Clone the project from GitHub\nSwitch to or create the branch that you wish work on (see GitFlow).\nStart Julia at your development folder.\nInstantiate the package (i.e., download and install the package dependencies).","category":"page"},{"location":"man/contributing/","page":"Contributing","title":"Contributing","text":"For example, you can get the package and startup Julia with","category":"page"},{"location":"man/contributing/","page":"Contributing","title":"Contributing","text":"git clone git@github.com:AP6YC/AdaptiveResonance.jl.git\njulia --project=.","category":"page"},{"location":"man/contributing/","page":"Contributing","title":"Contributing","text":"note: Note\nIn Julia, you must activate your project in the current REPL to point to the location/scope of installed packages. The above immediately activates the project when starting up Julia, but you may also separately startup the julia and activate the package with the interactive package manager via the ] syntax:julia\njulia> ]\n(@v1.6) pkg> activate .\n(AdaptiveResonance) pkg>","category":"page"},{"location":"man/contributing/","page":"Contributing","title":"Contributing","text":"You may run the package's unit tests after the above setup in Julia with","category":"page"},{"location":"man/contributing/","page":"Contributing","title":"Contributing","text":"julia> using Pkg\njulia> Pkg.instantiate()\njulia> Pkg.test()","category":"page"},{"location":"man/contributing/","page":"Contributing","title":"Contributing","text":"or interactively though the Julia package manager with","category":"page"},{"location":"man/contributing/","page":"Contributing","title":"Contributing","text":"julia> ]\n(AdaptiveResonance) pkg> instantiate\n(AdaptiveResonance) pkg> test","category":"page"},{"location":"man/contributing/#GitFlow","page":"Contributing","title":"GitFlow","text":"","category":"section"},{"location":"man/contributing/","page":"Contributing","title":"Contributing","text":"As of verson 0.3.7, the AdaptiveResonance.jl package follows the GitFlow git working model. The original post by Vincent Driessen outlines this methodology quite well, while Atlassian has a good tutorial as well. In summary:","category":"page"},{"location":"man/contributing/","page":"Contributing","title":"Contributing","text":"Create a feature branch off of the develop branch with the name feature/<my-feature-name>.\nCommit your changes and push to this feature branch.\nWhen you are satisfied with your changes, initiate a GitHub pull request (PR) to merge the feature branch with develop.\nIf the unit tests pass, the feature branch will first be merged with develop and then be deleted.\nReleases will be periodically initiated from the develop branch and versioned onto the master branch.\nImmediate bug fixes circumvent this process through a hotfix branch off of master.","category":"page"},{"location":"man/contributing/#Development-Details","page":"Contributing","title":"Development Details","text":"","category":"section"},{"location":"man/contributing/#Documentation","page":"Contributing","title":"Documentation","text":"","category":"section"},{"location":"man/contributing/","page":"Contributing","title":"Contributing","text":"These docs are currently hosted as a static site on the GitHub pages platform. They are setup to be built and served in a separate branch gh-pages from the master/development branch of the project.","category":"page"},{"location":"man/contributing/#Package-Structure","page":"Contributing","title":"Package Structure","text":"","category":"section"},{"location":"man/contributing/","page":"Contributing","title":"Contributing","text":"The AdaptiveResonance.jl package has the following file structure:","category":"page"},{"location":"man/contributing/","page":"Contributing","title":"Contributing","text":"AdaptiveResonance\n├── .github/workflows       // GitHub: workflows for testing and documentation.\n├── data                    // Data: CI data location.\n├── docs                    // Docs: documentation for the module.\n│   └───src                 //      Documentation source files.\n├── examples                // Source: example usage scripts.\n├── src                     // Source: majority of source code.\n│   ├───ART                 //      ART-based unsupervised modules.\n│   └───ARTMAP              //      ARTMAP-based supervised modules.\n├── test                    // Test: Unit, integration, and environment tests.\n├── .appveyor               // Appveyor: Windows-specific coverage.\n├── .gitignore              // Git: .gitignore for the whole project.\n├── LICENSE                 // Doc: the license to the project.\n├── Project.toml            // Julia: the Pkg.jl dependencies of the project.\n└── README.md               // Doc: this document.","category":"page"},{"location":"man/contributing/","page":"Contributing","title":"Contributing","text":"ART and ARTMAP algorithms are put into their own files within the src/ART/ and src/ARTMAP/ directories, respectively. Both of these directories have an \"index\" file where each module is \"included\" (i.e., src/ART/ART.jl), which is in turn \"included\" in the package module file src/AdaptiveResonance.jl.","category":"page"},{"location":"man/contributing/","page":"Contributing","title":"Contributing","text":"Abstract types and common structures/methods are included at the top of the package module file. All public methods and structs (i.e., for the end user) are \"exported\" at the end of this file.","category":"page"},{"location":"man/contributing/#ART-Module-Workflow","page":"Contributing","title":"ART Module Workflow","text":"","category":"section"},{"location":"man/contributing/","page":"Contributing","title":"Contributing","text":"To write an ART module for this project, it will require the following:","category":"page"},{"location":"man/contributing/","page":"Contributing","title":"Contributing","text":"A train! and classify method (within the module).\nAn keyword-options struct using the Parameters.jl macro @with_kw with assertions to keep the parameters within correct ranges.\nThree constructors:\nA default constructor (i.e. DDVFA()).\nA keyword argument constructor (passing the kwargs to the options struct defined above).\nA constructor with the options struct passed itself.\nUse of common type aliases in method definitions.\nAn internal DataConfig for setting up the data configuration, especially with data_setup! (src/common.jl).\nAn update_iter evaluation for each iteration (src/common.jl).\nInclusion to the correct ART index file (i.e., src/ART/ART.jl).\nExports of the names for the options and module constructors in the module definition (src/AdaptiveResonance.jl).","category":"page"},{"location":"man/contributing/#DataConfig","page":"Contributing","title":"DataConfig","text":"","category":"section"},{"location":"man/contributing/","page":"Contributing","title":"Contributing","text":"The original implementation of ART1 uses binary vectors, which have guaranteed separation between distinct vectors. Real-valued ART modules, however, face the problem of permitting vectors to be arbitrarily close to one another. Therefore, nearly every real-valued ART module uses [0, 1] normalization and complement-coding. This is reflected in the DataConfig struct in the common file src/common.jl.","category":"page"},{"location":"man/contributing/#Type-Aliases","page":"Contributing","title":"Type Aliases","text":"","category":"section"},{"location":"man/contributing/","page":"Contributing","title":"Contributing","text":"In the pursuit of an architecture-agnostic implementation (i.e., support for both 32- and 64-bit systems), type aliases and other special Julia types are used in this project.","category":"page"},{"location":"man/contributing/","page":"Contributing","title":"Contributing","text":"This module borrows a convention from the StatsBase.jl package by defining a variety of aliases for numerical types used throughout the package to standardize usage. This has the benefits of readability and speed by explicitly These are defined in src/common.jl and are currently as follows:","category":"page"},{"location":"man/contributing/","page":"Contributing","title":"Contributing","text":"# Real-numbered aliases\nconst RealArray{T<:Real, N} = AbstractArray{T, N}\nconst RealVector{T<:Real} = AbstractArray{T, 1}\nconst RealMatrix{T<:Real} = AbstractArray{T, 2}\n\n# Integered aliases\nconst IntegerArray{T<:Integer, N} = AbstractArray{T, N}\nconst IntegerVector{T<:Integer} = AbstractArray{T, 1}\nconst IntegerMatrix{T<:Integer} = AbstractArray{T, 2}\n\n# Specifically floating-point aliases\nconst RealFP = Union{Float32, Float64}","category":"page"},{"location":"man/contributing/","page":"Contributing","title":"Contributing","text":"In this package, data samples are always Real-valued (with the notable exception of ART1), while class labels are integered. Furthermore, independent class labels are always Int because of the Julia native support for a given system's signed native integer type.","category":"page"},{"location":"man/contributing/","page":"Contributing","title":"Contributing","text":"This project does not currently test for the support of arbitrary precision arithmetic because learning algorithms in general do not have a significant need for precision beyond even 32-bit floats.","category":"page"},{"location":"man/contributing/#Authors","page":"Contributing","title":"Authors","text":"","category":"section"},{"location":"man/contributing/","page":"Contributing","title":"Contributing","text":"If you simply have suggestions for improvement, Sasha Petrenko (<sap625@mst.edu>) is the current developer and maintainer of the AdaptiveResonance.jl package, so please feel free to reach out with thoughts and questions.","category":"page"},{"location":"man/guide/#Package-Guide","page":"Guide","title":"Package Guide","text":"","category":"section"},{"location":"man/guide/","page":"Guide","title":"Guide","text":"The AdaptiveResonance.jl package is built upon ART modules that contain all of the state information during training and inference. The ART modules are driven by options, which are themselves mutable keyword argument structs from the Parameters.jl package.","category":"page"},{"location":"man/guide/","page":"Guide","title":"Guide","text":"To work with AdaptiveResonance.jl, you should know:","category":"page"},{"location":"man/guide/","page":"Guide","title":"Guide","text":"How to install the package\nART module basics\nHow to use ART module options\nART vs. ARTMAP","category":"page"},{"location":"man/guide/#installation","page":"Guide","title":"Installation","text":"","category":"section"},{"location":"man/guide/","page":"Guide","title":"Guide","text":"The AdaptiveResonance package can be installed using the Julia package manager. From the Julia REPL, type ] to enter the Pkg REPL mode and run","category":"page"},{"location":"man/guide/","page":"Guide","title":"Guide","text":"pkg> add AdaptiveResonance","category":"page"},{"location":"man/guide/","page":"Guide","title":"Guide","text":"Alternatively, it can be added to your environment in a script with","category":"page"},{"location":"man/guide/","page":"Guide","title":"Guide","text":"using Pkg\nPkg.add(\"AdaptiveResonance\")","category":"page"},{"location":"man/guide/","page":"Guide","title":"Guide","text":"If you wish to have the latest changes between releases, you can directly add the GitHub repo as a dependency with","category":"page"},{"location":"man/guide/","page":"Guide","title":"Guide","text":"pkg> add https://github.com/AP6YC/AdaptiveResonance.jl","category":"page"},{"location":"man/guide/#art_modules","page":"Guide","title":"ART Modules","text":"","category":"section"},{"location":"man/guide/","page":"Guide","title":"Guide","text":"To work with ART modules, you should know:","category":"page"},{"location":"man/guide/","page":"Guide","title":"Guide","text":"Their basic methods\nIncremental vs. batch modes\nSupervised vs. unsupervised learning modes\nMismatch vs. Best-Matching-Unit","category":"page"},{"location":"man/guide/#methods","page":"Guide","title":"Methods","text":"","category":"section"},{"location":"man/guide/","page":"Guide","title":"Guide","text":"Every ART module is equipped with several constructors, a training function train!, and a classification/inference function classify. ART models are mutable structs, and they can be instantiated with","category":"page"},{"location":"man/guide/","page":"Guide","title":"Guide","text":"art = DDVFA()","category":"page"},{"location":"man/guide/","page":"Guide","title":"Guide","text":"For more ways to customize instantiation, see the ART options section.","category":"page"},{"location":"man/guide/","page":"Guide","title":"Guide","text":"To train and test these models, you use the train! and classify functions upon the models. Because training changes the internal parameters of the ART models and classification does not, train! uses an exclamation point while classify does not, following Julia standard usage.","category":"page"},{"location":"man/guide/","page":"Guide","title":"Guide","text":"For example, we may load data of some sort and train/test like so:","category":"page"},{"location":"man/guide/","page":"Guide","title":"Guide","text":"# Load the data from some source with a train/test split\ntrain_x, train_y, test_x, test_y = load_some_data()\n\n# Instantiate an arbitrary ART module\nart = DDVFA()\n\n# Train the module on the training data, getting the prescribed cluster labels\ny_hat_train = train!(art, train_x)\n\n# Conduct inference\ny_hat_test = classify(art, test_x)","category":"page"},{"location":"man/guide/","page":"Guide","title":"Guide","text":"note: Note\nBecause Julia arrays are column-major in memory, the AdaptiveResonance.jl package follows the Julia convention of assuming 2-D data arrays are in the shape of (n_features, n_samples).","category":"page"},{"location":"man/guide/#incremental_vs_batch","page":"Guide","title":"Incremental vs. Batch","text":"","category":"section"},{"location":"man/guide/","page":"Guide","title":"Guide","text":"This training and testing may be done in either incremental or batch modes:","category":"page"},{"location":"man/guide/","page":"Guide","title":"Guide","text":"# Create a destination container for the incremental examples\nn_train = length(train_y)\nn_test = length(test_y)\ny_hat_train_incremental = zeros(Integer, n_train)\ny_hat_test_incremental = zeros(Integer, n_test)\n\n# Loop over all training samples\nfor i = 1:n_train\n    y_hat_train_incremental[i] = train!(art, train_x[:, i])\nend\n\n# loop over all testing samples\nfor i = 1:n_test\n    y_hat_test_incremental[i] = classify(art, test_x[:, i])\nend","category":"page"},{"location":"man/guide/","page":"Guide","title":"Guide","text":"This is done through checking the dimensionality of the inputs. For example, if a matrix (i.e., 2-D array) is passed to the train! function, then the data is assumed to be (n_features, n_samples), and the module is trained on all samples. However, if the data is a vector (i.e., 1-D array), then the vector is interpreted as a single sample.","category":"page"},{"location":"man/guide/","page":"Guide","title":"Guide","text":"When supervised (see supervised vs. unsupervised), the dimensions of the labels must correspond to the dimensions of the data. For example, a 2-D matrix of the data must accompany a 1-D vector of labels, while a 1-D vector of a single data sample must accompany a single integer label.","category":"page"},{"location":"man/guide/","page":"Guide","title":"Guide","text":"Batch and incremental modes can be used interchangably after module instantiation.","category":"page"},{"location":"man/guide/","page":"Guide","title":"Guide","text":"note: Note\nThe first time that an ART module is trained, it infers the data parameters (e.g., feature dimensions, feature ranges, etc.) to setup the internal data configuration. This happens automatically in batch mode, but it cannot happen if the module is only trained incrementally. If you know the dimensions and minimum/maximum values of the features and want to train incrementally, you can use the function data_setup! after module instantiation, which can be used a number of ways. If you have the batch data available, you can set up with# Manually setup the data config with the data itself\ndata_setup!(art.config, data.train_x)If you do not have the batch data available, you can directly create a DataConfig with the minimums and maximums (inferring the number of features from the lengths of these vectors):# Get the mins and maxes vectors with some method\nmins, maxes = get_some_data_mins_maxes()\n\n# Directly update the data config\nart.config = DataConfig(mins, maxes)If all of the features share the same minimums and maximums, then you can use them as long as you specify the number of features:# Get the global minimum, maximum, and feature dimension somehow\nmin, max, dim = get_some_data_characteristics()\n\n# Directly update the data config with these global values\nart.config = DataConfig(min, max, dim)","category":"page"},{"location":"man/guide/#supervised_vs_unsupervised","page":"Guide","title":"Supervised vs. Unsupervised","text":"","category":"section"},{"location":"man/guide/","page":"Guide","title":"Guide","text":"ARTMAP modules require a supervised label argument because their formulations typically map internal cluster categories to labels:","category":"page"},{"location":"man/guide/","page":"Guide","title":"Guide","text":"# Create an arbitrary ARTMAP module\nartmap = DAM()\n\n# Conduct supervised learning\ny_hat_train = train!(artmap, train_x, train_y)\n\n# Conduct inference\ny_hat_test = classify(artmap, test_x)","category":"page"},{"location":"man/guide/","page":"Guide","title":"Guide","text":"In the case of ARTMAP, the returned training labels y_hat_train will always match the training labels train_y by definition. In addition to the classification accuracy (ranging from 0 to 1), you can test that the training labels match with the function performance:","category":"page"},{"location":"man/guide/","page":"Guide","title":"Guide","text":"# Verify that the training labels match\nperf_train = performance(y_hat_train, train_y)\n\n# Get the classification accuracy\nperf_test = performance(y_hat_test, test_y)","category":"page"},{"location":"man/guide/","page":"Guide","title":"Guide","text":"However, many ART modules, though unsupervised by definition, can also be trained in a supervised way by naively mapping categories to labels (more in ART vs. ARTMAP).","category":"page"},{"location":"man/guide/#mismatch-bmu","page":"Guide","title":"Mismatch vs. Best-Matching-Unit","text":"","category":"section"},{"location":"man/guide/","page":"Guide","title":"Guide","text":"During inference, ART algorithms report the category that satisfies the match/vigilance criterion (see Background). By default, in the case that no category satisfies this criterion the module reports a mismatch as -1. In modules that support it, a keyword argument get_bmu (default is false) can be used in the classify method to get the \"best-matching unit\", which is the category that maximizes the activation. This can be interpreted as the \"next-best guess\" of the model in the case that the sample is sufficiently different from anything that the model has seen. For example,","category":"page"},{"location":"man/guide/","page":"Guide","title":"Guide","text":"# Conduct inference, getting the best-matching unit in case of complete mismatch\ny_hat_bmu = classify(my_art, test_x, get_bmu=true)","category":"page"},{"location":"man/guide/#art_options","page":"Guide","title":"ART Options","text":"","category":"section"},{"location":"man/guide/","page":"Guide","title":"Guide","text":"The AdaptiveResonance package is designed for maximum flexibility for scientific research, even though this may come at the cost of learning instability if misused. Because of the diversity of ART modules, the package is structured around instantiating separate modules and using them for training and inference. Due to this diversity, each module has its own options struct with keyword arguments. These options have default values driven by standards in their respective literatures, so the ART modules may be used immediately without any customization. Furthermore, these options are mutable, so they may be modified before module instantiation, before training, or even after training.","category":"page"},{"location":"man/guide/","page":"Guide","title":"Guide","text":"For example, you can get going with the default options by creating an ART module with the default constructor:","category":"page"},{"location":"man/guide/","page":"Guide","title":"Guide","text":"my_art = DDVFA()","category":"page"},{"location":"man/guide/","page":"Guide","title":"Guide","text":"If you want to change the parameters before construction, you can create an options struct, modify it, then instantiate your ART module with it:","category":"page"},{"location":"man/guide/","page":"Guide","title":"Guide","text":"my_art_opts = opts_DDVFA()\nmy_art_opts.gamma = 3\nmy_art = DDVFA(my_art_opts)","category":"page"},{"location":"man/guide/","page":"Guide","title":"Guide","text":"The options are objects from the Parameters.jl project, so they can be instantiated even with keyword arguments:","category":"page"},{"location":"man/guide/","page":"Guide","title":"Guide","text":"my_art_opts = opts_DDVFA(gamma = 3)","category":"page"},{"location":"man/guide/","page":"Guide","title":"Guide","text":"note: Note\nAs of version 0.3.6, you can pass these keyword arguments directly to the ART model when constructing it withmy_art = DDVFA(gamma = 3)","category":"page"},{"location":"man/guide/","page":"Guide","title":"Guide","text":"You can even modify the parameters on the fly after the ART module has been instantiated by directly modifying the options within the module:","category":"page"},{"location":"man/guide/","page":"Guide","title":"Guide","text":"my_art = DDVFA()\nmy_art.opts.gamma = 3","category":"page"},{"location":"man/guide/","page":"Guide","title":"Guide","text":"Because of the @assert feature of the Parameters.jl package, each parameter is forced to lie within certain bounds by definition in the literature during options instantiation. However, it is possible to change these parameter values beyond their predefined bounds after instantiation.","category":"page"},{"location":"man/guide/","page":"Guide","title":"Guide","text":"note: Note\nYou must be careful when changing option values during or after training, as it may result in some undefined behavior. Modify the ART module options after instantiation at your own risk and discretion.","category":"page"},{"location":"man/guide/","page":"Guide","title":"Guide","text":"Though most parameters differ between each ART and ARTMAP module, they all share some quality-of-life options and parameters shared by all ART algorithms:","category":"page"},{"location":"man/guide/","page":"Guide","title":"Guide","text":"display::Bool: a flag to display or suppress progress bars and logging messages during training and testing.\nmax_epochs::Integer: the maximum number of epochs to train over the data, regardless if other stopping conditions have not been met yet.","category":"page"},{"location":"man/guide/","page":"Guide","title":"Guide","text":"Otherwise, most ART and ARTMAP modules share the following nomenclature for algorithmic parameters:","category":"page"},{"location":"man/guide/","page":"Guide","title":"Guide","text":"rho::Float: ART vigilance parameter [0, 1].\nalpha::Float: Choice parameter > 0.\nbeta::Float: Learning parameter (0, 1].\nepsilon::Float: Match tracking parameter (0, 1).","category":"page"},{"location":"man/guide/#art_vs_artmap","page":"Guide","title":"ART vs. ARTMAP","text":"","category":"section"},{"location":"man/guide/","page":"Guide","title":"Guide","text":"ART modules are generally unsupervised in formulation, so they do not explicitly require supervisory labels to their training examples. However, many of these modules can be formulated in the simplified ARTMAP style whereby the ART B module has a vigilance parameter of 1, directly mapping the categories of the ART A module to any provided supervisory labels.","category":"page"},{"location":"man/guide/","page":"Guide","title":"Guide","text":"This is done in the training stage through the optional argument y=...:","category":"page"},{"location":"man/guide/","page":"Guide","title":"Guide","text":"# Create an arbitrary ART module\nart = DDVFA()\n\n# Naively prescribe supervised labels to cluster categories\ny_hat_train = train!(art, train_x, y=train_y)","category":"page"},{"location":"man/guide/","page":"Guide","title":"Guide","text":"This can also be done incrementally with the same function:","category":"page"},{"location":"man/guide/","page":"Guide","title":"Guide","text":"# Get the number of training samples and create a results container\nn_train = length(train_y)\ny_hat_train_incremental = zeros(Integer, n_train)\n\n# Train incrementally over all training samples\nfor i = 1:n_train\n    y_hat_train_incremental[i] = train!(art, train_x[:, i], y=train_y[i])\nend","category":"page"},{"location":"man/guide/","page":"Guide","title":"Guide","text":"Without provided labels, the ART modules behave as expected, incrementally creating categories when necessary during the training phase.","category":"page"},{"location":"examples/art/ddvfa_supervised/#ddvfa_supervised","page":"Supervised DDVFA Example","title":"Supervised DDVFA Example","text":"","category":"section"},{"location":"examples/art/ddvfa_supervised/","page":"Supervised DDVFA Example","title":"Supervised DDVFA Example","text":"(Image: Source code) (Image: notebook) (Image: compat) (Image: Author) (Image: Update time)","category":"page"},{"location":"examples/art/ddvfa_supervised/","page":"Supervised DDVFA Example","title":"Supervised DDVFA Example","text":"DDVFA is an unsupervised clustering algorithm by definition, but it can be adaptived for supervised learning by mapping the module's internal categories to the true labels. ART modules such as DDVFA can also be used in simple supervised mode where provided labels are used in place of internal incremental labels for the clusters, providing a method of assessing the clustering performance when labels are available.","category":"page"},{"location":"examples/art/ddvfa_supervised/","page":"Supervised DDVFA Example","title":"Supervised DDVFA Example","text":"We begin with importing AdaptiveResonance for the ART modules and MLDatasets for some data utilities.","category":"page"},{"location":"examples/art/ddvfa_supervised/","page":"Supervised DDVFA Example","title":"Supervised DDVFA Example","text":"using AdaptiveResonance # ART\nusing MLDatasets        # Iris dataset\nusing MLDataUtils       # Shuffling and splitting\nusing Printf            # Formatted number printing","category":"page"},{"location":"examples/art/ddvfa_supervised/","page":"Supervised DDVFA Example","title":"Supervised DDVFA Example","text":"We will download the Iris dataset for its small size and benchmark use for clustering algorithms.","category":"page"},{"location":"examples/art/ddvfa_supervised/","page":"Supervised DDVFA Example","title":"Supervised DDVFA Example","text":"Iris.download(i_accept_the_terms_of_use=true)\nfeatures, labels = Iris.features(), Iris.labels()","category":"page"},{"location":"examples/art/ddvfa_supervised/","page":"Supervised DDVFA Example","title":"Supervised DDVFA Example","text":"Because the MLDatasets package gives us Iris labels as strings, we will use the MLDataUtils.convertlabel method with the MLLabelUtils.LabelEnc.Indices type to get a list of integers representing each class:","category":"page"},{"location":"examples/art/ddvfa_supervised/","page":"Supervised DDVFA Example","title":"Supervised DDVFA Example","text":"labels = convertlabel(LabelEnc.Indices{Int}, labels)\nunique(labels)","category":"page"},{"location":"examples/art/ddvfa_supervised/","page":"Supervised DDVFA Example","title":"Supervised DDVFA Example","text":"Next, we will create a train/test split with the MLDataUtils.stratifiedobs utility:","category":"page"},{"location":"examples/art/ddvfa_supervised/","page":"Supervised DDVFA Example","title":"Supervised DDVFA Example","text":"(X_train, y_train), (X_test, y_test) = stratifiedobs((features, labels))","category":"page"},{"location":"examples/art/ddvfa_supervised/","page":"Supervised DDVFA Example","title":"Supervised DDVFA Example","text":"Now, we can create our DDVFA module. We'll do so with the default contstructor, though the module itself has many options that you can alter during instantiation.","category":"page"},{"location":"examples/art/ddvfa_supervised/","page":"Supervised DDVFA Example","title":"Supervised DDVFA Example","text":"art = DDVFA()","category":"page"},{"location":"examples/art/ddvfa_supervised/","page":"Supervised DDVFA Example","title":"Supervised DDVFA Example","text":"We can train the model in batch mode upon the data in a simple supervised mode. We do so by passing the integer vector of labels to the training method with the simple keyword y. Just as in unsupervised training, we can extract the module's prescribed labels from the training method, which should match up to the training labels as we will see later.","category":"page"},{"location":"examples/art/ddvfa_supervised/","page":"Supervised DDVFA Example","title":"Supervised DDVFA Example","text":"# Train in simple supervised mode by passing the labels as a keyword argument.\ny_hat_train = train!(art, X_train, y=y_train)\nprintln(\"Training labels: \",  size(y_hat_train), \" \", typeof(y_hat_train))","category":"page"},{"location":"examples/art/ddvfa_supervised/","page":"Supervised DDVFA Example","title":"Supervised DDVFA Example","text":"We can classify the testing data to see how we generalize. At the same time, we can see the effect of getting the best-matching unit in the case of complete mismatch (see the docs on Mismatch vs. BMU)","category":"page"},{"location":"examples/art/ddvfa_supervised/","page":"Supervised DDVFA Example","title":"Supervised DDVFA Example","text":"# Classify both ways\ny_hat = AdaptiveResonance.classify(art, X_test)\ny_hat_bmu = AdaptiveResonance.classify(art, X_test, get_bmu=true)\n\n# Check the shape and type of the output labels\nprintln(\"Testing labels: \",  size(y_hat), \" \", typeof(y_hat))\nprintln(\"Testing labels with bmu: \",  size(y_hat_bmu), \" \", typeof(y_hat_bmu))","category":"page"},{"location":"examples/art/ddvfa_supervised/","page":"Supervised DDVFA Example","title":"Supervised DDVFA Example","text":"Finally, we can calculate the performances (number correct over total) of the model upon all three regimes:","category":"page"},{"location":"examples/art/ddvfa_supervised/","page":"Supervised DDVFA Example","title":"Supervised DDVFA Example","text":"Training data\nTesting data\nTesting data with get_bmu=true","category":"page"},{"location":"examples/art/ddvfa_supervised/","page":"Supervised DDVFA Example","title":"Supervised DDVFA Example","text":"# Calculate performance on training data, testing data, and with get_bmu\nperf_train = performance(y_hat_train, y_train)\nperf_test = performance(y_hat, y_test)\nperf_test_bmu = performance(y_hat_bmu, y_test)\n\n# Format each performance number for comparison\n@printf \"Training performance: %.4f\\n\" perf_train\n@printf \"Testing performance: %.4f\\n\" perf_test\n@printf \"Best-matching unit testing performance: %.4f\\n\" perf_test_bmu","category":"page"},{"location":"examples/art/ddvfa_supervised/","page":"Supervised DDVFA Example","title":"Supervised DDVFA Example","text":"","category":"page"},{"location":"examples/art/ddvfa_supervised/","page":"Supervised DDVFA Example","title":"Supervised DDVFA Example","text":"This page was generated using DemoCards.jl and Literate.jl.","category":"page"},{"location":"man/modules/#Modules","page":"Modules","title":"Modules","text":"","category":"section"},{"location":"man/modules/","page":"Modules","title":"Modules","text":"This project implements a number of ART-based models with options that modulate their behavior (see the options section of the Guide)","category":"page"},{"location":"man/modules/","page":"Modules","title":"Modules","text":"This page lists both the implemented models and some variants","category":"page"},{"location":"man/modules/#Implemented-Models","page":"Modules","title":"Implemented Models","text":"","category":"section"},{"location":"man/modules/","page":"Modules","title":"Modules","text":"This project has implementations of the following ART (unsupervised) and ARTMAP (supervised) modules:","category":"page"},{"location":"man/modules/","page":"Modules","title":"Modules","text":"ART\nFuzzyART: Fuzzy ART\nDVFA: Dual Vigilance Fuzzy ART\nDDVFA: Distributed Dual Vigilance Fuzzy ART\nARTMAP\nSFAM: Simplified Fuzzy ARTMAP\nFAM: Fuzzy ARTMAP\nDAM: Default ARTMAP","category":"page"},{"location":"man/modules/#Variants","page":"Modules","title":"Variants","text":"","category":"section"},{"location":"man/modules/","page":"Modules","title":"Modules","text":"Each module contains many options that modulate its behavior. Some of these options are used to modulate the internals of the module, such as switching the match and activation functions, to achieve different modules that are found in the literature.","category":"page"},{"location":"man/modules/","page":"Modules","title":"Modules","text":"These variants are:","category":"page"},{"location":"man/modules/","page":"Modules","title":"Modules","text":"Gamma-Normalized FuzzyART","category":"page"},{"location":"man/modules/#Gamma-Normalized-FuzzyART","page":"Modules","title":"Gamma-Normalized FuzzyART","text":"","category":"section"},{"location":"man/modules/","page":"Modules","title":"Modules","text":"A Gamma-Normalized FuzzyART is a FuzzyART module where the gamma normalization option is set on gamma_normalization=true and the kernel width parameter is set to gamma = 10 (gamma_ref is 1.0 by default):","category":"page"},{"location":"man/modules/","page":"Modules","title":"Modules","text":"my_gnfa = FuzzyART(gamma_normalization=true, gamma=5.0)","category":"page"},{"location":"man/modules/","page":"Modules","title":"Modules","text":"The gamma_normalization flag must be set high here because it also changes the thresholding value and match function of the module.","category":"page"},{"location":"examples/art/ddvfa_unsupervised/#ddvfa_unsupervised","page":"Unsupervised DDVFA Example","title":"Unsupervised DDVFA Example","text":"","category":"section"},{"location":"examples/art/ddvfa_unsupervised/","page":"Unsupervised DDVFA Example","title":"Unsupervised DDVFA Example","text":"(Image: Source code) (Image: notebook) (Image: compat) (Image: Author) (Image: Update time)","category":"page"},{"location":"examples/art/ddvfa_unsupervised/","page":"Unsupervised DDVFA Example","title":"Unsupervised DDVFA Example","text":"DDVFA is an unsupervised clustering algorithm by definition, so it can be used to cluster a set of samples all at once in batch mode.","category":"page"},{"location":"examples/art/ddvfa_unsupervised/","page":"Unsupervised DDVFA Example","title":"Unsupervised DDVFA Example","text":"We begin with importing AdaptiveResonance for the ART modules and MLDatasets for loading some data.","category":"page"},{"location":"examples/art/ddvfa_unsupervised/","page":"Unsupervised DDVFA Example","title":"Unsupervised DDVFA Example","text":"using AdaptiveResonance\nusing MLDatasets","category":"page"},{"location":"examples/art/ddvfa_unsupervised/","page":"Unsupervised DDVFA Example","title":"Unsupervised DDVFA Example","text":"We will download the Iris dataset for its small size and benchmark use for clustering algorithms.","category":"page"},{"location":"examples/art/ddvfa_unsupervised/","page":"Unsupervised DDVFA Example","title":"Unsupervised DDVFA Example","text":"Iris.download(i_accept_the_terms_of_use=true)\nfeatures, labels = Iris.features(), Iris.labels()","category":"page"},{"location":"examples/art/ddvfa_unsupervised/","page":"Unsupervised DDVFA Example","title":"Unsupervised DDVFA Example","text":"Next, we will instantiate a DDVFA module. We could create an options struct for reuse with opts=opts_DDVFA(...), but for now we will use the direct keyword arguments approach.","category":"page"},{"location":"examples/art/ddvfa_unsupervised/","page":"Unsupervised DDVFA Example","title":"Unsupervised DDVFA Example","text":"art = DDVFA(rho_lb=0.6, rho_ub=0.75)","category":"page"},{"location":"examples/art/ddvfa_unsupervised/","page":"Unsupervised DDVFA Example","title":"Unsupervised DDVFA Example","text":"To train the module on the training data, we use train!. The train method returns the prescribed cluster labels, which are just what the algorithm believes are unique/separate cluster. This is because we are doing unsupervised learning rather than supervised learning with known labels.","category":"page"},{"location":"examples/art/ddvfa_unsupervised/","page":"Unsupervised DDVFA Example","title":"Unsupervised DDVFA Example","text":"y_hat_train = train!(art, features)","category":"page"},{"location":"examples/art/ddvfa_unsupervised/","page":"Unsupervised DDVFA Example","title":"Unsupervised DDVFA Example","text":"Though we could inspect the unique entries in the list above, we can see the number of categories directly from the art module.","category":"page"},{"location":"examples/art/ddvfa_unsupervised/","page":"Unsupervised DDVFA Example","title":"Unsupervised DDVFA Example","text":"art.n_categories","category":"page"},{"location":"examples/art/ddvfa_unsupervised/","page":"Unsupervised DDVFA Example","title":"Unsupervised DDVFA Example","text":"Because DDVFA actually has FuzzyART modules for F2 nodes, each category has its own category prototypes. We can see the total number of weights in the DDVFA module by summing n_categories across all F2 nodes.","category":"page"},{"location":"examples/art/ddvfa_unsupervised/","page":"Unsupervised DDVFA Example","title":"Unsupervised DDVFA Example","text":"total_vec = [art.F2[i].n_categories for i = 1:art.n_categories]\ntotal_cat = sum(total_vec)","category":"page"},{"location":"examples/art/ddvfa_unsupervised/","page":"Unsupervised DDVFA Example","title":"Unsupervised DDVFA Example","text":"","category":"page"},{"location":"examples/art/ddvfa_unsupervised/","page":"Unsupervised DDVFA Example","title":"Unsupervised DDVFA Example","text":"This page was generated using DemoCards.jl and Literate.jl.","category":"page"},{"location":"examples/#examples","page":"Examples","title":"Examples","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"This section contains some examples using the AdaptiveResonance.jl package with topics ranging from how to the internals of package work to practical examples on different datasets.","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"These examples are separated into the following sections:","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"AdaptiveResonance: examples exploring the various components of the package, such as how the options structs work and how to train incremental vs batch modes.\nART: examples using ART modules in unsupervised and simple supervised modes.\nARTMAP: examples using ARTMAP modules for supervised learning.","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"","category":"page"},{"location":"examples/#AdaptiveResonance","page":"Examples","title":"AdaptiveResonance","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"These examples demonstrate different aspects of usage that are common to all modules in the package, such as incremental vs. batch learning and how to use module options.","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"<div class=\"grid-card-section\">","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"<div class=\"card grid-card\">\n<div class=\"grid-card-cover\">\n<div class=\"grid-card-description\">","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"This demo illustrates how the data configuration object works for data preprocessing in ART modules that require it.","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"</div>","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"(Image: card-cover-image)","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"</div>\n<div class=\"grid-card-text\">","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"ART DataConfig Example","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"</div>\n</div>","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"<div class=\"card grid-card\">\n<div class=\"grid-card-cover\">\n<div class=\"grid-card-description\">","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"This demo illustrates how to use incremental training methods vs. batch training for all ART modules.","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"</div>","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"(Image: card-cover-image)","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"</div>\n<div class=\"grid-card-text\">","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"Incremental vs. Batch Example","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"</div>\n</div>","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"<div class=\"card grid-card\">\n<div class=\"grid-card-cover\">\n<div class=\"grid-card-description\">","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"This demo illustrates how to use options and modify the options for all ART and ARTMAP modules.","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"</div>","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"(Image: card-cover-image)","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"</div>\n<div class=\"grid-card-text\">","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"ART Options Example","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"</div>\n</div>","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"</div>","category":"page"},{"location":"examples/#ART","page":"Examples","title":"ART","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"All ART modules learn in an unsupervised (i.e. clustering) mode by default, but they all can accept labels in the simplified ARTMAP fashion (see the Package Guide).","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"<div class=\"grid-card-section\">","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"<div class=\"card grid-card\">\n<div class=\"grid-card-cover\">\n<div class=\"grid-card-description\">","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"This demo shows how to use DDVFA for simple supervised learning by clustering Iris samples and mapping the modules internal categories to the true labels.","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"</div>","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"(Image: card-cover-image)","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"</div>\n<div class=\"grid-card-text\">","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"Supervised DDVFA Example","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"</div>\n</div>","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"<div class=\"card grid-card\">\n<div class=\"grid-card-cover\">\n<div class=\"grid-card-description\">","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"This demo shows how to use DDVFA for unsupervised learning by clustering Iris samples.","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"</div>","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"(Image: card-cover-image)","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"</div>\n<div class=\"grid-card-text\">","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"Unsupervised DDVFA Example","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"</div>\n</div>","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"</div>","category":"page"},{"location":"examples/#ARTMAP","page":"Examples","title":"ARTMAP","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"ARTMAP modules are supervised by definition, so they require supervised labels in the training stage. These examples demonstrate different use-cases with ARTMAP modules.","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"<div class=\"grid-card-section\">","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"<div class=\"card grid-card\">\n<div class=\"grid-card-cover\">\n<div class=\"grid-card-description\">","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"This demo shows how to use a Simplified FuzzyARTMAP (SFAM) module to conduct supervised learning on the Iris dataset.","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"</div>","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"(Image: card-cover-image)","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"</div>\n<div class=\"grid-card-text\">","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"Supervised Simplified FuzzyARTMAP (SFAM) Example","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"</div>\n</div>","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"</div>","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"","category":"page"},{"location":"examples/adaptive_resonance/options/#options","page":"ART Options Example","title":"ART Options Example","text":"","category":"section"},{"location":"examples/adaptive_resonance/options/","page":"ART Options Example","title":"ART Options Example","text":"(Image: Source code) (Image: notebook) (Image: compat) (Image: Author) (Image: Update time)","category":"page"},{"location":"examples/adaptive_resonance/options/#Overview","page":"ART Options Example","title":"Overview","text":"","category":"section"},{"location":"examples/adaptive_resonance/options/","page":"ART Options Example","title":"ART Options Example","text":"The AdaptiveResonance.jl package has several ways of handling options for ART modules. These methods are meant to give maximum flexibility to the user for sharing and interpreting options, which themselves vary between each module.","category":"page"},{"location":"examples/adaptive_resonance/options/","page":"ART Options Example","title":"ART Options Example","text":"note: Note\nFor more info on options in ART modules, see the guide in the docs on ART options.","category":"page"},{"location":"examples/adaptive_resonance/options/#ART-Options","page":"ART Options Example","title":"ART Options","text":"","category":"section"},{"location":"examples/adaptive_resonance/options/","page":"ART Options Example","title":"ART Options Example","text":"To get a feel for the ART options system, we will inspect different options and their instantiation methods.","category":"page"},{"location":"examples/adaptive_resonance/options/#Inspection","page":"ART Options Example","title":"Inspection","text":"","category":"section"},{"location":"examples/adaptive_resonance/options/","page":"ART Options Example","title":"ART Options Example","text":"First, we load AdaptiveResonance:","category":"page"},{"location":"examples/adaptive_resonance/options/","page":"ART Options Example","title":"ART Options Example","text":"using AdaptiveResonance","category":"page"},{"location":"examples/adaptive_resonance/options/","page":"ART Options Example","title":"ART Options Example","text":"Every ART module has a default constructor, which can be instantiated in the usual way:","category":"page"},{"location":"examples/adaptive_resonance/options/","page":"ART Options Example","title":"ART Options Example","text":"# Create a FuzzyART module with default options\nmy_fuzzyart = FuzzyART()\ntypeof(my_fuzzyart)","category":"page"},{"location":"examples/adaptive_resonance/options/","page":"ART Options Example","title":"ART Options Example","text":"Within every ART module is a Parameters.jl struct named opts containing the options for the module","category":"page"},{"location":"examples/adaptive_resonance/options/","page":"ART Options Example","title":"ART Options Example","text":"# Check the FuzzyART options\nmy_fuzzyart.opts","category":"page"},{"location":"examples/adaptive_resonance/options/","page":"ART Options Example","title":"ART Options Example","text":"Note that the options here have the type opts_FuzzyART. This nomenclature is used throughout the module to indicate an options type associated with an ART module. For example, the options for a DDVFA module are opts_DDVFA:","category":"page"},{"location":"examples/adaptive_resonance/options/","page":"ART Options Example","title":"ART Options Example","text":"# Create a DDVFA module and check the type of the options\nmy_ddvfa = DDVFA()\ntypeof(my_ddvfa.opts)","category":"page"},{"location":"examples/adaptive_resonance/options/","page":"ART Options Example","title":"ART Options Example","text":"In fact, we can create an instance of these options with a default constructor:","category":"page"},{"location":"examples/adaptive_resonance/options/","page":"ART Options Example","title":"ART Options Example","text":"# Create a separate options struct\nmy_fuzzyart_opts = opts_FuzzyART()","category":"page"},{"location":"examples/adaptive_resonance/options/","page":"ART Options Example","title":"ART Options Example","text":"In addition to the default constructor, we can construct ART modules by instantiating these options and passing them to the module during construction:","category":"page"},{"location":"examples/adaptive_resonance/options/","page":"ART Options Example","title":"ART Options Example","text":"# Instantiate an ART module by passing our options\nmy_fuzzyart = FuzzyART(my_fuzzyart_opts)\nmy_other_fuzzyart = FuzzyART(my_fuzzyart_opts)","category":"page"},{"location":"examples/adaptive_resonance/options/#Specifying-Options","page":"ART Options Example","title":"Specifying Options","text":"","category":"section"},{"location":"examples/adaptive_resonance/options/","page":"ART Options Example","title":"ART Options Example","text":"Now to the good stuff: because of the behavior of the Parameters.jl type, each option has a default value that we can modify during instantiation with keyword arguments:","category":"page"},{"location":"examples/adaptive_resonance/options/","page":"ART Options Example","title":"ART Options Example","text":"# Change some of the default FuzzyART options\nmy_fuzzyart_opts = opts_FuzzyART(\n    rho=0.6,\n    gamma_normalization=true\n)\nmy_fuzzyart = FuzzyART(my_fuzzyart_opts)","category":"page"},{"location":"examples/adaptive_resonance/options/","page":"ART Options Example","title":"ART Options Example","text":"As some syntactic sugar, we can pass these keyword arguments directly to the module during instantiation if we have no need to share option structs:","category":"page"},{"location":"examples/adaptive_resonance/options/","page":"ART Options Example","title":"ART Options Example","text":"# Pass these keyword arguments to the module directly\nmy_fuzzyart = FuzzyART(\n    rho=0.6,\n    gamma_normalization=true\n)","category":"page"},{"location":"examples/adaptive_resonance/options/","page":"ART Options Example","title":"ART Options Example","text":"Before training, we can also instantiate the model and alter the options afterward:","category":"page"},{"location":"examples/adaptive_resonance/options/","page":"ART Options Example","title":"ART Options Example","text":"my_fuzzyart = FuzzyART()\nmy_fuzzyart.opts.rho=0.6","category":"page"},{"location":"examples/adaptive_resonance/options/","page":"ART Options Example","title":"ART Options Example","text":"note: Note\nAll ART modules are designed to use this options struct internally when the parameters are needed. It is possible to change these parameters in the middle of training and evaluation, but some algorithmic instability may occur.","category":"page"},{"location":"examples/adaptive_resonance/options/#Comparison","page":"ART Options Example","title":"Comparison","text":"","category":"section"},{"location":"examples/adaptive_resonance/options/","page":"ART Options Example","title":"ART Options Example","text":"To see the effect that changing these parameters has on the modules, we can train and test them side-by-side.","category":"page"},{"location":"examples/adaptive_resonance/options/","page":"ART Options Example","title":"ART Options Example","text":"We begin with importing AdaptiveResonance for the ART modules and MLDatasets for some data utilities.","category":"page"},{"location":"examples/adaptive_resonance/options/","page":"ART Options Example","title":"ART Options Example","text":"using MLDatasets        # Iris dataset\nusing MLDataUtils       # Shuffling and splitting\nusing Printf            # Formatted number printing\nusing MultivariateStats # Principal component analysis (PCA)\nusing Plots             # Plotting frontend\npyplot()                # Use PyPlot backend","category":"page"},{"location":"examples/adaptive_resonance/options/","page":"ART Options Example","title":"ART Options Example","text":"We will download the Iris dataset for its small size and benchmark use for clustering algorithms.","category":"page"},{"location":"examples/adaptive_resonance/options/","page":"ART Options Example","title":"ART Options Example","text":"Iris.download(i_accept_the_terms_of_use=true)\nfeatures, labels = Iris.features(), Iris.labels()","category":"page"},{"location":"examples/adaptive_resonance/options/","page":"ART Options Example","title":"ART Options Example","text":"Because the MLDatasets package gives us Iris labels as strings, we will use the MLDataUtils.convertlabel method with the MLLabelUtils.LabelEnc.Indices type to get a list of integers representing each class:","category":"page"},{"location":"examples/adaptive_resonance/options/","page":"ART Options Example","title":"ART Options Example","text":"labels = convertlabel(LabelEnc.Indices{Int}, labels)\nunique(labels)","category":"page"},{"location":"examples/adaptive_resonance/options/","page":"ART Options Example","title":"ART Options Example","text":"Next, we will create a train/test split with the MLDataUtils.stratifiedobs utility:","category":"page"},{"location":"examples/adaptive_resonance/options/","page":"ART Options Example","title":"ART Options Example","text":"(X_train, y_train), (X_test, y_test) = stratifiedobs((features, labels))","category":"page"},{"location":"examples/adaptive_resonance/options/","page":"ART Options Example","title":"ART Options Example","text":"Now we can create several FuzzyART modules with different options.","category":"page"},{"location":"examples/adaptive_resonance/options/","page":"ART Options Example","title":"ART Options Example","text":"# Create two FuzzyARTs with different vigilance values and suppressing logging messages\nrho_1 = 0.5\nrho_2 = 0.7\nmy_fuzzyart_1 = FuzzyART(rho=rho_1, display=false)\nmy_fuzzyart_2 = FuzzyART(rho=rho_2, display=false)","category":"page"},{"location":"examples/adaptive_resonance/options/","page":"ART Options Example","title":"ART Options Example","text":"Here, we will train these FuzzyART modules in simple supervised mode by passing the supervised labels as a keyword argument:","category":"page"},{"location":"examples/adaptive_resonance/options/","page":"ART Options Example","title":"ART Options Example","text":"# Train in simple supervised mode by passing the labels as a keyword argument.\ny_hat_train_1 = train!(my_fuzzyart_1, X_train, y=y_train)\ny_hat_train_2 = train!(my_fuzzyart_2, X_train, y=y_train)","category":"page"},{"location":"examples/adaptive_resonance/options/","page":"ART Options Example","title":"ART Options Example","text":"We then classify the test data with both modules:","category":"page"},{"location":"examples/adaptive_resonance/options/","page":"ART Options Example","title":"ART Options Example","text":"y_hat_1 = AdaptiveResonance.classify(my_fuzzyart_1, X_test, get_bmu=true)\ny_hat_2 = AdaptiveResonance.classify(my_fuzzyart_2, X_test, get_bmu=true)\n\n# Check the shape and type of the output labels\nprintln(\"FuzzyART 1 labels: \",  size(y_hat_1), \" \", typeof(y_hat_1))\nprintln(\"FuzzyART 2 labels: \",  size(y_hat_2), \" \", typeof(y_hat_2))\n\n# Calculate the performance on the test data\nperf_test_1 = performance(y_hat_1, y_test)\nperf_test_2 = performance(y_hat_2, y_test)\n\n# Format each performance number for comparison\n@printf \"Testing performance rho=%.1f: %.4f\\n\" rho_1 perf_test_1\n@printf \"Testing performance rho=%.1f: %.4f\\n\" rho_2 perf_test_2","category":"page"},{"location":"examples/adaptive_resonance/options/","page":"ART Options Example","title":"ART Options Example","text":"In addition to having different performances, we can see that there is a subsequent trade-off in the number of categories used:","category":"page"},{"location":"examples/adaptive_resonance/options/","page":"ART Options Example","title":"ART Options Example","text":"# Print the number of categories for each vigilance parameter\n@printf \"Number of categories rho=%.1f: %i\\n\" rho_1 my_fuzzyart_1.n_categories\n@printf \"Number of categories rho=%.1f: %i\\n\" rho_2 my_fuzzyart_2.n_categories","category":"page"},{"location":"examples/adaptive_resonance/options/","page":"ART Options Example","title":"ART Options Example","text":"The variation between vigilance parameter, number of categories created during learning, and testing performance/generalization is a central theme in ART-based algorithms.","category":"page"},{"location":"examples/adaptive_resonance/options/#Visualization","page":"ART Options Example","title":"Visualization","text":"","category":"section"},{"location":"examples/adaptive_resonance/options/","page":"ART Options Example","title":"ART Options Example","text":"Now, to visualize how the two models differ in how they partition the data, we can use principal component analysis (PCA) to compress to two plotting dimensions. PCA is a method to represent a dataset in a different number of dimensions while preserving the relative separation between datapoints. Though most datasets are not able to be effectively transformed down to two dimensions, this technique is useful to get a general sense of how well separated the classes are and how well your algorithm classifies them.","category":"page"},{"location":"examples/adaptive_resonance/options/","page":"ART Options Example","title":"ART Options Example","text":"# Train a PCA model to visually separate the features in two dimensions.\nM = fit(PCA, features; maxoutdim=2)\n\n# Apply the PCA model to the testing set\nX_test_pca = transform(M, X_test)","category":"page"},{"location":"examples/adaptive_resonance/options/","page":"ART Options Example","title":"ART Options Example","text":"We can now plot the PCA'ed test set and label them according to the two FuzzyART's","category":"page"},{"location":"examples/adaptive_resonance/options/","page":"ART Options Example","title":"ART Options Example","text":"# Create the two scatterplot objects\np1 = scatter(\n    X_test_pca[1, :],\n    X_test_pca[2, :],\n    group=y_hat_1,\n    markersize=8,\n    title=@sprintf \"FuzzyART \\$\\\\rho\\$ = %.1f\" rho_1\n)\np2 = scatter(\n    X_test_pca[1, :],   # PCA dimension 1\n    X_test_pca[2, :],   # PCA dimension 2\n    group = y_hat_2,    # labels belonging to each point\n    markersize = 8,     # size of scatter points\n    title=@sprintf \"FuzzyART \\$\\\\rho\\$ = %.1f\" rho_2    # formatted title\n)\n\n# Plot the two scatterplots together\nplot(\n    p1, p2,                 # scatterplot objects\n    layout = (1, 2),        # plot side-by-side\n    legend = false,         # no legend\n    xtickfontsize = 12,     # x-tick size\n    ytickfontsize = 12,     # y-tick size\n    dpi = 300,              # Set the dots-per-inch\n    xlims = :round,         # Round up the x-limits to the nearest whole number\n    xlabel = \"\\$PCA_1\\$\",   # x-label\n    ylabel = \"\\$PCA_2\\$\",   # y-label\n)","category":"page"},{"location":"examples/adaptive_resonance/options/","page":"ART Options Example","title":"ART Options Example","text":"We can see that the two different vigilance values result in similar resutls on the whole, though they differ in how they classify certain samples that straddle the border between","category":"page"},{"location":"examples/adaptive_resonance/options/","page":"ART Options Example","title":"ART Options Example","text":"png(\"assets/options-cover\") #hide","category":"page"},{"location":"examples/adaptive_resonance/options/","page":"ART Options Example","title":"ART Options Example","text":"","category":"page"},{"location":"examples/adaptive_resonance/options/","page":"ART Options Example","title":"ART Options Example","text":"This page was generated using DemoCards.jl and Literate.jl.","category":"page"},{"location":"examples/adaptive_resonance/data_config/#data_config","page":"ART DataConfig Example","title":"ART DataConfig Example","text":"","category":"section"},{"location":"examples/adaptive_resonance/data_config/","page":"ART DataConfig Example","title":"ART DataConfig Example","text":"(Image: Source code) (Image: notebook) (Image: compat) (Image: Author) (Image: Update time)","category":"page"},{"location":"examples/adaptive_resonance/data_config/#Overview","page":"ART DataConfig Example","title":"Overview","text":"","category":"section"},{"location":"examples/adaptive_resonance/data_config/","page":"ART DataConfig Example","title":"ART DataConfig Example","text":"In their derivations, ART modules have some special requirements when it comes to their input features. FuzzyART in particular, and subsequently its derivatives, has a requirement that the inputs be bounded and complement coded. This is due to some consequences such as weight decay that occur when using real-valued patterns rather than binary ones (and hence operations like fuzzy membership).","category":"page"},{"location":"examples/adaptive_resonance/data_config/","page":"ART DataConfig Example","title":"ART DataConfig Example","text":"Preprocessing of the features occurs as follows:","category":"page"},{"location":"examples/adaptive_resonance/data_config/","page":"ART DataConfig Example","title":"ART DataConfig Example","text":"The features are linearly normalized from 0 to 1 with respect to each feature with linear_normalization. This is done according to some known bounds that each feature has.\nThe features are then complement coded, meaning that the feature vector is appended to its 1-complement (i.e., x rightarrow leftx 1-xright) with complement_code.","category":"page"},{"location":"examples/adaptive_resonance/data_config/","page":"ART DataConfig Example","title":"ART DataConfig Example","text":"This preprocessing has the ultimate consequence that the input features must be bounded. This many not be a problem in some offline applications with a fixed dataset, but in others where the bounds are not known, techniques such as sigmoidal limiting are often used to place an artificial limit.","category":"page"},{"location":"examples/adaptive_resonance/data_config/#DataConfig","page":"ART DataConfig Example","title":"DataConfig","text":"","category":"section"},{"location":"examples/adaptive_resonance/data_config/","page":"ART DataConfig Example","title":"ART DataConfig Example","text":"Regardless, this process requires some a-priori knowledge about the minimums and maximums that each feature can have, which is stored as a preprocessing configuration. This preprocessing configuration is saved in every ART module as a DataConfig object called config, which we can see is uninitialized at first:","category":"page"},{"location":"examples/adaptive_resonance/data_config/","page":"ART DataConfig Example","title":"ART DataConfig Example","text":"# Load the library\nusing AdaptiveResonance\n\n# Create a new ART module and inspect its uninitialized data config `config`\nart = FuzzyART()\nart.config","category":"page"},{"location":"examples/adaptive_resonance/data_config/","page":"ART DataConfig Example","title":"ART DataConfig Example","text":"We see that the type of art.config is DataConfig. We can see what the internal elements of this struct are with fieldnames:","category":"page"},{"location":"examples/adaptive_resonance/data_config/","page":"ART DataConfig Example","title":"ART DataConfig Example","text":"fieldnames(AdaptiveResonance.DataConfig)","category":"page"},{"location":"examples/adaptive_resonance/data_config/","page":"ART DataConfig Example","title":"ART DataConfig Example","text":"We see that the dataconfig has a boolean setup flag, minimum and maximum feature vectors, dimensionality of the data, and the complement coded dimensionality (twice the size of the original dimension).","category":"page"},{"location":"examples/adaptive_resonance/data_config/#Automatic-Configuration","page":"ART DataConfig Example","title":"Automatic Configuration","text":"","category":"section"},{"location":"examples/adaptive_resonance/data_config/","page":"ART DataConfig Example","title":"ART DataConfig Example","text":"In batch training mode, the minimums and maximums are detected automatically; the minimum and maximum values for every feature are saved and used for the preprocessing step at every subsequent iteration.","category":"page"},{"location":"examples/adaptive_resonance/data_config/","page":"ART DataConfig Example","title":"ART DataConfig Example","text":"# Load data\nusing MLDatasets\n\n# We will download the Iris dataset for its small size and benchmark use for clustering algorithms.\nIris.download(i_accept_the_terms_of_use=true)\nfeatures, labels = Iris.features(), Iris.labels()\n\n# We will then train the FuzzyART module in unsupervised mode and see that the data config is now set\ny_hat_train = train!(art, features)\nart.config","category":"page"},{"location":"examples/adaptive_resonance/data_config/","page":"ART DataConfig Example","title":"ART DataConfig Example","text":"note: Note\nThis automatic detection of feature characteristics only occurs if the config is not already setup. If it is setup beforehand, then that config is used instead.","category":"page"},{"location":"examples/adaptive_resonance/data_config/#Manual-Configuration","page":"ART DataConfig Example","title":"Manual Configuration","text":"","category":"section"},{"location":"examples/adaptive_resonance/data_config/","page":"ART DataConfig Example","title":"ART DataConfig Example","text":"As mentioned before, we may not always have the luxury of having a representative dataset in advance. Alternatively, we may know the bounds of the features but wish to run incrementally rather than in batch. In these cases, we can setup the config the various DataConfig constructors.","category":"page"},{"location":"examples/adaptive_resonance/data_config/","page":"ART DataConfig Example","title":"ART DataConfig Example","text":"For example, if the features are all bounded from -1 to 1, we have to also specify the original dimension of the data in DataConfig(min, max, dim):","category":"page"},{"location":"examples/adaptive_resonance/data_config/","page":"ART DataConfig Example","title":"ART DataConfig Example","text":"# Reinitialize the FuzzyART module\nart = FuzzyART()\n# Tell the module that we have 20 features all ranging from -1 to 1\nart.config = DataConfig(-1, 1, 20)","category":"page"},{"location":"examples/adaptive_resonance/data_config/","page":"ART DataConfig Example","title":"ART DataConfig Example","text":"If the features differ in their ranges, we can specify with DataConfig(mins, maxs):","category":"page"},{"location":"examples/adaptive_resonance/data_config/","page":"ART DataConfig Example","title":"ART DataConfig Example","text":"# Assume some minimum and maximum values for each feature\nmins = [-1,-2,-1.5]\nmaxs = [3, 2, 1]\nart.config = DataConfig(mins, maxs)","category":"page"},{"location":"examples/adaptive_resonance/data_config/","page":"ART DataConfig Example","title":"ART DataConfig Example","text":"Here, we don't need to specify the feature dimensionality because it is inferred from the length of the range values.","category":"page"},{"location":"examples/adaptive_resonance/data_config/","page":"ART DataConfig Example","title":"ART DataConfig Example","text":"note: Note\nAfter the first training run, the weights of the network are set to the size of the complement coded dimension. If you wish to change the dimension of the features, you will need to create a new network.","category":"page"},{"location":"examples/adaptive_resonance/data_config/","page":"ART DataConfig Example","title":"ART DataConfig Example","text":"","category":"page"},{"location":"examples/adaptive_resonance/data_config/","page":"ART DataConfig Example","title":"ART DataConfig Example","text":"This page was generated using DemoCards.jl and Literate.jl.","category":"page"},{"location":"examples/artmap/sfam_iris/#sfam_iris","page":"Supervised Simplified FuzzyARTMAP (SFAM) Example","title":"Supervised Simplified FuzzyARTMAP (SFAM) Example","text":"","category":"section"},{"location":"examples/artmap/sfam_iris/","page":"Supervised Simplified FuzzyARTMAP (SFAM) Example","title":"Supervised Simplified FuzzyARTMAP (SFAM) Example","text":"(Image: Source code) (Image: notebook) (Image: compat) (Image: Author) (Image: Update time)","category":"page"},{"location":"examples/artmap/sfam_iris/","page":"Supervised Simplified FuzzyARTMAP (SFAM) Example","title":"Supervised Simplified FuzzyARTMAP (SFAM) Example","text":"SFAM is a supervised algorithm by definition, so we use it to map a set of features to a set of supervisory labels. We will do so by training and testing on the ubiquitous Iris dataset and seeing how well the SFAM module generalizes the data.","category":"page"},{"location":"examples/artmap/sfam_iris/","page":"Supervised Simplified FuzzyARTMAP (SFAM) Example","title":"Supervised Simplified FuzzyARTMAP (SFAM) Example","text":"We begin with importing AdaptiveResonance for the ART modules and MLDatasets for some data utilities.","category":"page"},{"location":"examples/artmap/sfam_iris/","page":"Supervised Simplified FuzzyARTMAP (SFAM) Example","title":"Supervised Simplified FuzzyARTMAP (SFAM) Example","text":"using AdaptiveResonance # ART\nusing MLDatasets        # Iris dataset\nusing MLDataUtils       # Shuffling and splitting\nusing Printf            # Formatted number printing","category":"page"},{"location":"examples/artmap/sfam_iris/","page":"Supervised Simplified FuzzyARTMAP (SFAM) Example","title":"Supervised Simplified FuzzyARTMAP (SFAM) Example","text":"We will download the Iris dataset for its small size and benchmark use for clustering algorithms.","category":"page"},{"location":"examples/artmap/sfam_iris/","page":"Supervised Simplified FuzzyARTMAP (SFAM) Example","title":"Supervised Simplified FuzzyARTMAP (SFAM) Example","text":"Iris.download(i_accept_the_terms_of_use=true)\nfeatures, labels = Iris.features(), Iris.labels()","category":"page"},{"location":"examples/artmap/sfam_iris/","page":"Supervised Simplified FuzzyARTMAP (SFAM) Example","title":"Supervised Simplified FuzzyARTMAP (SFAM) Example","text":"Because the MLDatasets package gives us Iris labels as strings, we will use the MLDataUtils.convertlabel method with the MLLabelUtils.LabelEnc.Indices type to get a list of integers representing each class:","category":"page"},{"location":"examples/artmap/sfam_iris/","page":"Supervised Simplified FuzzyARTMAP (SFAM) Example","title":"Supervised Simplified FuzzyARTMAP (SFAM) Example","text":"labels = convertlabel(LabelEnc.Indices{Int}, labels)\nunique(labels)","category":"page"},{"location":"examples/artmap/sfam_iris/","page":"Supervised Simplified FuzzyARTMAP (SFAM) Example","title":"Supervised Simplified FuzzyARTMAP (SFAM) Example","text":"Next, we will create a train/test split with the MLDataUtils.stratifiedobs utility:","category":"page"},{"location":"examples/artmap/sfam_iris/","page":"Supervised Simplified FuzzyARTMAP (SFAM) Example","title":"Supervised Simplified FuzzyARTMAP (SFAM) Example","text":"(X_train, y_train), (X_test, y_test) = stratifiedobs((features, labels))","category":"page"},{"location":"examples/artmap/sfam_iris/","page":"Supervised Simplified FuzzyARTMAP (SFAM) Example","title":"Supervised Simplified FuzzyARTMAP (SFAM) Example","text":"Now, we can create our SFAM module. We'll do so with the default contstructor, though the module itself has many options that can be altered during instantiation.","category":"page"},{"location":"examples/artmap/sfam_iris/","page":"Supervised Simplified FuzzyARTMAP (SFAM) Example","title":"Supervised Simplified FuzzyARTMAP (SFAM) Example","text":"# Create the SFAM module\nart = SFAM()\n\n# Change the match tracking parameter after instantiation\nart.opts.epsilon = 1e-2","category":"page"},{"location":"examples/artmap/sfam_iris/","page":"Supervised Simplified FuzzyARTMAP (SFAM) Example","title":"Supervised Simplified FuzzyARTMAP (SFAM) Example","text":"We can train the model in batch mode upon the data and supervisory labels. We do so by directly passing the integer vector of labels to the training method. Just as in other modules, we can extract the SFAM's prescribed labels from the training method, which should match up to the training labels as we will see later.","category":"page"},{"location":"examples/artmap/sfam_iris/","page":"Supervised Simplified FuzzyARTMAP (SFAM) Example","title":"Supervised Simplified FuzzyARTMAP (SFAM) Example","text":"# Train in supervised mode by directly passing the labels.\ny_hat_train = train!(art, X_train, y_train)\nprintln(\"Training labels: \",  size(y_hat_train), \" \", typeof(y_hat_train))","category":"page"},{"location":"examples/artmap/sfam_iris/","page":"Supervised Simplified FuzzyARTMAP (SFAM) Example","title":"Supervised Simplified FuzzyARTMAP (SFAM) Example","text":"We can classify the testing data to see how we generalize. At the same time, we can see the effect of getting the best-matching unit in the case of complete mismatch (see the docs on Mismatch vs. BMU)","category":"page"},{"location":"examples/artmap/sfam_iris/","page":"Supervised Simplified FuzzyARTMAP (SFAM) Example","title":"Supervised Simplified FuzzyARTMAP (SFAM) Example","text":"# Classify both ways\ny_hat = AdaptiveResonance.classify(art, X_test)\ny_hat_bmu = AdaptiveResonance.classify(art, X_test, get_bmu=true)\n\n# Check the shape and type of the output labels\nprintln(\"Testing labels: \",  size(y_hat), \" \", typeof(y_hat))\nprintln(\"Testing labels with bmu: \",  size(y_hat_bmu), \" \", typeof(y_hat_bmu))","category":"page"},{"location":"examples/artmap/sfam_iris/","page":"Supervised Simplified FuzzyARTMAP (SFAM) Example","title":"Supervised Simplified FuzzyARTMAP (SFAM) Example","text":"Finally, we can calculate the performances (number correct over total) of the model upon all three regimes:","category":"page"},{"location":"examples/artmap/sfam_iris/","page":"Supervised Simplified FuzzyARTMAP (SFAM) Example","title":"Supervised Simplified FuzzyARTMAP (SFAM) Example","text":"Training data\nTesting data\nTesting data with get_bmu=true","category":"page"},{"location":"examples/artmap/sfam_iris/","page":"Supervised Simplified FuzzyARTMAP (SFAM) Example","title":"Supervised Simplified FuzzyARTMAP (SFAM) Example","text":"# Calculate performance on training data, testing data, and with get_bmu\nperf_train = performance(y_hat_train, y_train)\nperf_test = performance(y_hat, y_test)\nperf_test_bmu = performance(y_hat_bmu, y_test)\n\n# Format each performance number for comparison\n@printf \"Training performance: %.4f\\n\" perf_train\n@printf \"Testing performance: %.4f\\n\" perf_test\n@printf \"Best-matching unit testing performance: %.4f\\n\" perf_test_bmu","category":"page"},{"location":"examples/artmap/sfam_iris/","page":"Supervised Simplified FuzzyARTMAP (SFAM) Example","title":"Supervised Simplified FuzzyARTMAP (SFAM) Example","text":"","category":"page"},{"location":"examples/artmap/sfam_iris/","page":"Supervised Simplified FuzzyARTMAP (SFAM) Example","title":"Supervised Simplified FuzzyARTMAP (SFAM) Example","text":"This page was generated using DemoCards.jl and Literate.jl.","category":"page"},{"location":"getting-started/whatisart/#Background","page":"Background","title":"Background","text":"","category":"section"},{"location":"getting-started/whatisart/","page":"Background","title":"Background","text":"This page provides a theoretical overview of Adaptive Resonance Theory and what this project aims to accomplish.","category":"page"},{"location":"getting-started/whatisart/#What-is-Adaptive-Resonance-Theory?","page":"Background","title":"What is Adaptive Resonance Theory?","text":"","category":"section"},{"location":"getting-started/whatisart/","page":"Background","title":"Background","text":"Adaptive Resonance Theory (commonly abbreviated to ART) is both a neurological theory and a family of neurogenitive neural network models for machine learning.","category":"page"},{"location":"getting-started/whatisart/","page":"Background","title":"Background","text":"ART began as a neurocognitive theory of how fields of cells can continuously learn stable representations, and it evolved into the basis for a myriad of practical machine learning algorithms. Pioneered by Stephen Grossberg and Gail Carpenter, the field has had contributions across many years and from many disciplines, resulting in a plethora of engineering applications and theoretical advancements that have enabled ART-based algorithms to compete with many other modern learning and clustering algorithms.","category":"page"},{"location":"getting-started/whatisart/","page":"Background","title":"Background","text":"Because of the high degree of interplay between the neurocognitive theory and the engineering models born of it, the term ART is frequently used to refer to both in the modern day (for better or for worse).","category":"page"},{"location":"getting-started/whatisart/","page":"Background","title":"Background","text":"Stephen Grossberg's has recently released a book summarizing the work of him, his wife Gail Carpenter, and his colleagues on Adaptive Resonance Theory in his book Conscious Brain, Resonant Mind.","category":"page"},{"location":"getting-started/whatisart/#ART-Basics","page":"Background","title":"ART Basics","text":"","category":"section"},{"location":"getting-started/whatisart/","page":"Background","title":"Background","text":"(Image: art)","category":"page"},{"location":"getting-started/whatisart/#ART-Dynamics","page":"Background","title":"ART Dynamics","text":"","category":"section"},{"location":"getting-started/whatisart/","page":"Background","title":"Background","text":"Nearly every ART model shares a basic set of dynamics:","category":"page"},{"location":"getting-started/whatisart/","page":"Background","title":"Background","text":"ART models typically have two layers/fields denoted F1 and F2.\nThe F1 field is the feature representation field.  Most often, it is simply the input feature sample itself (after some necessary preprocessing).\nThe F2 field is the category representation field.  With some exceptions, each node in the F2 field generally represents its own category.  This is most easily understood as a weight vector representing a prototype for a class or centroid of a cluster.\nAn activation function is used to find the order of categories \"most activated\" for a given sample in F1.\nIn order of highest activation, a match function is used to compute the agreement between the sample and the categories.\nIf the match function for a category evaluates to a value above a threshold known as the vigilance parameter (rho), the weights of that category may be updated according to a learning rule.\nIf there is complete mismatch across all categories, then a new categories is created according to some instantiation rule.","category":"page"},{"location":"getting-started/whatisart/#ART-Considerations","page":"Background","title":"ART Considerations","text":"","category":"section"},{"location":"getting-started/whatisart/","page":"Background","title":"Background","text":"In addition to the dynamics typical of an ART model, you must know:","category":"page"},{"location":"getting-started/whatisart/","page":"Background","title":"Background","text":"ART models are inherently designed for unsupervised learning (i.e., learning in the absense of supervisory labels for samples).  This is also known as clustering.\nART models are capable of supervised learning and reinforcement learning through some redesign and/or combination of ART models.  For example, ARTMAP models are combinations of two ART models in a special way, one learning feature-to-category mappings and another learning category-to-label mappingss.  ART modules are used for reinforcement learning by representing the mappings between state, value, and action spaces with ART dynamics.\nAlmost all ART models face the problem of the appropriate selection of the vigilance parameter, which may depend in its optimality according to the problem.\nBeing a class of neurogenitive neural network models, ART models gain the ability for theoretically infinite capacity along with the problem of \"category proliferation,\" which is the undesirable increase in the number of categories as the model continues to learn, leading to increasing computational time.  In contrast, while the evaluation time of a deep neural network is always exactly the same, there exist upper bounds in their representational capacity.\nNearly every ART model requires feature normalization (i.e., feature elements lying within 01) and a process known as complement coding where the feature vector is appended to its vector complement 1-barx. This is because real-numbered vectors can be arbitrarily close to one another, hindering learning performance, which requires a degree of contrast enhancement between samples to ensure their separation.","category":"page"},{"location":"getting-started/whatisart/","page":"Background","title":"Background","text":"To learn about their implementations, nearly every practical ART model is listed in a recent ART survey paper by Leonardo Enzo Brito da Silva.","category":"page"},{"location":"getting-started/whatisart/#History-and-Development","page":"Background","title":"History and Development","text":"","category":"section"},{"location":"getting-started/whatisart/","page":"Background","title":"Background","text":"At a high level, ART began with a neural network model known as the Grossberg Network named after Stephen Grossberg. This network treats the firing of neurons in frequency domain as basic shunting models, which are recurrently connected to increase their own activity while suppressing the activities of others nearby (i.e., on-center, off-surround). Using this shunting model, Grossberg shows that autonomous, associative learning can occur with what are known as instar networks.","category":"page"},{"location":"getting-started/whatisart/","page":"Background","title":"Background","text":"By representing categories as a field of instar networks, new categories could be optimally learned by the instantiation of new neurons. However, it was shown that the learning stability of Grossberg Networks degrades as the number of represented categories increases. Discoveries in the neurocognitive theory and breakthroughs in their implementation led to the introduction of a recurrent connections between the two fields of the network to stabilize the learning. These breakthroughs were based upon the discovery that autonomous learning depends on the interplay and agreement between perception and expectation, frequently referred to as bottom-up and top-down processes. Furthermore, it is resonance between these states in the frequency domain that gives rise to conscious experiences and that permit adaptive weights to change, leading to the phenomenon of learning. The theory has many explanatory consequences in psychology, such as why attention is required for learning, but its consequences in the engineering models are that it stabilizes learning in cooperative-competitive dynamics, such as interconnected fields of neurons, which are most often chaotic.","category":"page"},{"location":"getting-started/whatisart/","page":"Background","title":"Background","text":"Chapters 18 and 19 of the book by Neural Network Design by Hagan, Demuth, Beale, and De Jesus provide a good theoretical basis for learning how these network models were eventually implemented into the first binary-vector implementation of ART1.","category":"page"},{"location":"man/full-index/#main-index","page":"Index","title":"Index","text":"","category":"section"},{"location":"man/full-index/","page":"Index","title":"Index","text":"This page lists the core methods and types of the AdaptiveResonance.jl package. The Methods section lists the public methods for the package that use the modules in Types. Each of these entries link to the docstrings in the Docs section.","category":"page"},{"location":"man/full-index/","page":"Index","title":"Index","text":"ART modules document their internal working parameters and references, while their hyperparameters/options are documented under their corresponding option structs opts_....","category":"page"},{"location":"man/full-index/#index-methods","page":"Index","title":"Methods","text":"","category":"section"},{"location":"man/full-index/","page":"Index","title":"Index","text":"Modules = [AdaptiveResonance]\nOrder = [:function]\nPublic = true","category":"page"},{"location":"man/full-index/#index-types","page":"Index","title":"Types","text":"","category":"section"},{"location":"man/full-index/","page":"Index","title":"Index","text":"Modules = [AdaptiveResonance]\nOrder = [:type]\nPublic = true","category":"page"},{"location":"man/full-index/#index-docs","page":"Index","title":"Docs","text":"","category":"section"},{"location":"man/full-index/","page":"Index","title":"Index","text":"AdaptiveResonance\ntrain!\nclassify\ndata_setup!\nperformance\ncomplement_code\nget_data_characteristics\nlinear_normalization\nget_data_shape\nget_n_samples\nDDVFA\nDVFA\nFuzzyART\nDAM\nFAM\nSFAM\nopts_DDVFA\nopts_DVFA\nopts_FuzzyART\nopts_DAM\nopts_FAM\nopts_SFAM\nDataConfig\nARTModule\nART\nARTMAP\nARTOpts","category":"page"},{"location":"man/full-index/#AdaptiveResonance","page":"Index","title":"AdaptiveResonance","text":"Main module for AdaptiveResonance.jl, a Julia package of adaptive resonance theory algorithms.\n\nThis module exports all of the ART modules, options, and utilities used by the AdaptiveResonance.jl package.\n\nExports\n\nART\nARTMAP\nARTModule\nARTOpts\nDAM\nDDVFA\nDVFA\nDataConfig\nFAM\nFuzzyART\nSFAM\nartscene_filter\nclassify\ncolor_to_gray\ncomplement_code\ncontrast_insensitive_oriented_filtering\ncontrast_normalization\ncontrast_sensitive_oriented_filtering\ndata_setup!\nget_W\nget_data_characteristics\nget_data_shape\nget_n_samples\nlinear_normalization\nopts_DAM\nopts_DDVFA\nopts_DVFA\nopts_FAM\nopts_FuzzyART\nopts_SFAM\norientation_competition\npatch_orientation_color\nperformance\ntrain!\n\n\n\n\n\n","category":"module"},{"location":"man/full-index/#AdaptiveResonance.train!","page":"Index","title":"AdaptiveResonance.train!","text":"train!(art::ARTMAP, x::RealMatrix, y::IntegerVector, preprocessed::Bool=false)\n\nTrain the ARTMAP model on a batch of data 'x' with supervisory labels 'y.'\n\nArguments\n\nart::ARTMAP: the supervised ARTMAP model to train.\nx::RealMatrix: the 2-D dataset containing columns of samples with rows of features.\ny::IntegerVector: labels for supervisory training.\npreprocessed::Bool=false: flag, if the data has already been complement coded or not.\n\n\n\n\n\ntrain!(art::ARTMAP, x::RealVector, y::Integer ; preprocessed::Bool=false)\n\nTrain the supervised ARTMAP model on a single sample of features 'x' with supervisory label 'y'.\n\nArguments\n\nart::ARTMAP: the supervised ART model to train.\nx::RealVector: the single sample feature vector to train upon.\ny::Integer: the label for supervisory training.\npreprocessed::Bool=false: optional, flag if the data has already been complement coded or not.\n\n\n\n\n\ntrain!(art::ART, x::RealMatrix ; y::IntegerVector=Vector{Int}(), preprocessed::Bool=false)\n\nTrain the ART model on a batch of data 'x' with optional supervisory labels 'y.'\n\nArguments\n\nart::ART: the unsupervised ART model to train.\nx::RealMatrix: the 2-D dataset containing columns of samples with rows of features.\ny::IntegerVector=Vector{Int}(): optional, labels for simple supervisory training.\npreprocessed::Bool=false: optional, flag if the data has already been complement coded or not.\n\n\n\n\n\ntrain!(art::ART, x::RealVector ; y::Integer=0, preprocessed::Bool=false)\n\nTrain the ART model on a single sample of features 'x' with an optional supervisory label.\n\nArguments\n\nart::ART: the unsupervised ART model to train.\nx::RealVector: the single sample feature vector to train upon.\ny::Integer=0: optional, a label for simple supervisory training.\npreprocessed::Bool=false: optional, flag if the data has already been complement coded or not.\n\n\n\n\n\n","category":"function"},{"location":"man/full-index/#AdaptiveResonance.classify","page":"Index","title":"AdaptiveResonance.classify","text":"classify(art::ARTModule, x::RealMatrix ; preprocessed::Bool=false, get_bmu::Bool=false)\n\nPredict categories of 'x' using the ART model.\n\nReturns predicted categories 'y_hat.'\n\nArguments\n\nart::ARTModule: ART or ARTMAP module to use for batch inference.\nx::RealMatrix: the 2-D dataset containing columns of samples with rows of features.\npreprocessed::Bool=false: flag, if the data has already been complement coded or not.\nget_bmu::Bool=false, flag, if the model should return the best-matching-unit label in the case of total mismatch.\n\nExamples\n\njulia> my_DDVFA = DDVFA()\nDDVFA\n    opts: opts_DDVFA\n    ...\njulia> x, y = load_data()\njulia> train!(my_DDVFA, x)\njulia> y_hat = classify(my_DDVFA, y)\n\n\n\n\n\nclassify(art::ARTModule, x::RealVector ; preprocessed::Bool=false, get_bmu::Bool=false)\n\nPredict categories of a single sample of features 'x' using the ART model.\n\nReturns predicted category 'y_hat.'\n\nArguments\n\nart::ARTModule: ART or ARTMAP module to use for batch inference.\nx::RealVector: the single sample of features to classify.\npreprocessed::Bool=false: optional, flag if the data has already been complement coded or not.\nget_bmu::Bool=false: optional, flag if the model should return the best-matching-unit label in the case of total mismatch.\n\n\n\n\n\n","category":"function"},{"location":"man/full-index/#AdaptiveResonance.data_setup!","page":"Index","title":"AdaptiveResonance.data_setup!","text":"data_setup!(config::DataConfig, data::RealMatrix)\n\nSets up the data config for the ART module before training.\n\n\n\n\n\ndata_setup!(art::ARTModule, data::RealMatrix)\n\nConvenience method for setting up the DataConfig of an ART module in advance.\n\n\n\n\n\n","category":"function"},{"location":"man/full-index/#AdaptiveResonance.performance","page":"Index","title":"AdaptiveResonance.performance","text":"performance(y_hat::IntegerVector, y::IntegerVector)\n\nReturns the categorization performance of y_hat against y.\n\n\n\n\n\n","category":"function"},{"location":"man/full-index/#AdaptiveResonance.complement_code","page":"Index","title":"AdaptiveResonance.complement_code","text":"complement_code(data::RealArray ; config::DataConfig=DataConfig())\n\nNormalize the data x to [0, 1] and returns the augmented vector [x, 1 - x].\n\n\n\n\n\n","category":"function"},{"location":"man/full-index/#AdaptiveResonance.get_data_characteristics","page":"Index","title":"AdaptiveResonance.get_data_characteristics","text":"get_data_characteristics(data::RealArray ; config::DataConfig=DataConfig())\n\nGet the characteristics of the data, taking account if a data config is passed.\n\nIf no DataConfig is passed, then the data characteristics come from the array itself. Otherwise, use the config for the statistics of the data and the data array for the number of samples.\n\n\n\n\n\n","category":"function"},{"location":"man/full-index/#AdaptiveResonance.linear_normalization","page":"Index","title":"AdaptiveResonance.linear_normalization","text":"linear_normalization(data::RealVector ; config::DataConfig=DataConfig())\n\nNormalize the data to the range [0, 1] along each feature.\n\n\n\n\n\nlinear_normalization(data::RealMatrix ; config::DataConfig=DataConfig())\n\nNormalize the data to the range [0, 1] along each feature.\n\n\n\n\n\n","category":"function"},{"location":"man/full-index/#AdaptiveResonance.get_data_shape","page":"Index","title":"AdaptiveResonance.get_data_shape","text":"get_data_shape(data::RealArray)\n\nReturns the correct feature dimension and number of samples.\n\n\n\n\n\n","category":"function"},{"location":"man/full-index/#AdaptiveResonance.get_n_samples","page":"Index","title":"AdaptiveResonance.get_n_samples","text":"get_n_samples(data::RealArray)\n\nReturns the number of samples, accounting for 1-D and 2-D arrays.\n\n\n\n\n\n","category":"function"},{"location":"man/full-index/#AdaptiveResonance.DDVFA","page":"Index","title":"AdaptiveResonance.DDVFA","text":"DDVFA <: ART\n\nDistributed Dual Vigilance Fuzzy ARTMAP module struct.\n\nFor module options, see AdaptiveResonance.opts_DDVFA.\n\nOption Parameters\n\nopts::opts_DDVFA: DDVFA options struct.\nsubopts::opts_FuzzyART: FuzzyART options struct used for all F2 nodes.\nconfig::DataConfig: data configuration struct.\n\nWorking Parameters\n\nthreshold::Float: operating module threshold value, a function of the vigilance parameter.\nF2::Vector{FuzzyART}: list of F2 nodes (themselves FuzzyART modules).\nlabels::IntegerVector: incremental list of labels corresponding to each F2 node, self-prescribed or supervised.\nn_categories::Int: number of total categories.\nepoch::Int: current training epoch.\nT::Float: winning activation value from most recent sample.\nM::Float: winning match value from most recent sample.\n\nReferences\n\nL. E. Brito da Silva, I. Elnabarawy, and D. C. Wunsch, “Distributed dual vigilance fuzzy adaptive resonance theory learns online, retrieves arbitrarily-shaped clusters, and mitigates order dependence,” Neural Networks, vol. 121, pp. 208-228, 2020, doi: 10.1016/j.neunet.2019.08.033.\nG. Carpenter, S. Grossberg, and D. Rosen, \"Fuzzy ART: Fast stable learning and categorization of analog patterns by an adaptive resonance system,\" Neural Networks, vol. 4, no. 6, pp. 759-771, 1991.\n\n\n\n\n\n","category":"type"},{"location":"man/full-index/#AdaptiveResonance.DVFA","page":"Index","title":"AdaptiveResonance.DVFA","text":"DVFA <: ART\n\nDual Vigilance Fuzzy ARTMAP module struct.\n\nFor module options, see AdaptiveResonance.opts_DVFA.\n\nOption Parameters\n\nopts::opts_DVFA: DVFA options struct.\nconfig::DataConfig: data configuration struct.\n\nWorking Parameters\n\nthreshold_ub::Float: operating upper bound module threshold value, a function of the upper bound vigilance parameter.\nthreshold_lb::Float: operating lower bound module threshold value, a function of the lower bound vigilance parameter.\nlabels::IntegerVector: incremental list of labels corresponding to each F2 node, self-prescribed or supervised.\nW::RealMatrix: category weight matrix.\nT::RealVector: activation values for every weight for a given sample.\nM::RealVector: match values for every weight for a given sample.\nn_categories::Int: number of category weights (F2 nodes).\nn_clusters::Int: number of labeled clusters, may be lower than n_categories\nepoch::Int: current training epoch.\n\nReferences:\n\nL. E. Brito da Silva, I. Elnabarawy and D. C. Wunsch II, \"Dual Vigilance Fuzzy ART,\" Neural Networks Letters. To appear.\nG. Carpenter, S. Grossberg, and D. Rosen, \"Fuzzy ART: Fast stable learning and categorization of analog patterns by an adaptive resonance system,\" Neural Networks, vol. 4, no. 6, pp. 759-771, 1991.\n\n\n\n\n\n","category":"type"},{"location":"man/full-index/#AdaptiveResonance.FuzzyART","page":"Index","title":"AdaptiveResonance.FuzzyART","text":"FuzzyART <: ART\n\nGamma-Normalized Fuzzy ART learner struct\n\nFor module options, see AdaptiveResonance.opts_FuzzyART.\n\nOption Parameters\n\nopts::opts_FuzzyART: FuzzyART options struct.\nconfig::DataConfig: data configuration struct.\n\nWorking Parameters\n\nthreshold::Float: operating module threshold value, a function of the vigilance parameter.\nlabels::IntegerVector: incremental list of labels corresponding to each F2 node, self-prescribed or supervised.\nT::RealVector: activation values for every weight for a given sample.\nM::RealVector: match values for every weight for a given sample.\nW::RealMatrix: category weight matrix.\nn_instance::IntegerVector: number of weights associated with each category.\nn_categories::Int: number of category weights (F2 nodes).\nepoch::Int: current training epoch.\n\nReferences\n\nG. Carpenter, S. Grossberg, and D. Rosen, \"Fuzzy ART: Fast stable learning and categorization of analog patterns by an adaptive resonance system,\" Neural Networks, vol. 4, no. 6, pp. 759-771, 1991.\n\n\n\n\n\n","category":"type"},{"location":"man/full-index/#AdaptiveResonance.DAM","page":"Index","title":"AdaptiveResonance.DAM","text":"DAM <: ARTMAP\n\nDefault ARTMAP struct.\n\nFor module options, see AdaptiveResonance.opts_DAM.\n\nOption Parameters\n\nopts::opts_DAM: Default ARTMAP options struct.\nconfig::DataConfig: data configuration struct.\n\nWorking Parameters\n\nW::RealMatrix: category weight matrix.\nlabels::IntegerVector: incremental list of labels corresponding to each F2 node, self-prescribed or supervised.\nn_categories::Int: number of category weights (F2 nodes).\nepoch::Int: current training epoch.\n\nReferences\n\nG. P. Amis and G. A. Carpenter, “Default ARTMAP 2,” IEEE Int. Conf. Neural Networks - Conf. Proc., vol. 2, no. September 2007, pp. 777-782, Mar. 2007, doi: 10.1109/IJCNN.2007.4371056.\n\n\n\n\n\n","category":"type"},{"location":"man/full-index/#AdaptiveResonance.FAM","page":"Index","title":"AdaptiveResonance.FAM","text":"FAM <: ARTMAP\n\nFuzzy ARTMAP struct.\n\nFor module options, see AdaptiveResonance.opts_FAM.\n\nOption Parameters\n\nopts::opts_FAM: Fuzzy ARTMAP options struct.\nconfig::DataConfig: data configuration struct.\n\nWorking Parameters\n\nW::RealMatrix: category weight matrix.\nlabels::IntegerVector: incremental list of labels corresponding to each F2 node, self-prescribed or supervised.\nn_categories::Int: number of category weights (F2 nodes).\nepoch::Int: current training epoch.\n\nReferences\n\nG. A. Carpenter, S. Grossberg, N. Markuzon, J. H. Reynolds, and D. B. Rosen, “Fuzzy ARTMAP: A Neural Network Architecture for Incremental Supervised Learning of Analog Multidimensional Maps,” IEEE Trans. Neural Networks, vol. 3, no. 5, pp. 698-713, 1992, doi: 10.1109/72.159059.\n\n\n\n\n\n","category":"type"},{"location":"man/full-index/#AdaptiveResonance.SFAM","page":"Index","title":"AdaptiveResonance.SFAM","text":"SFAM <: ARTMAP\n\nSimple Fuzzy ARTMAP struct.\n\nFor module options, see AdaptiveResonance.opts_SFAM.\n\nOption Parameters\n\nopts::opts_SFAM: Simplified Fuzzy ARTMAP options struct.\nconfig::DataConfig: data configuration struct.\n\nWorking Parameters\n\nW::RealMatrix: category weight matrix.\nlabels::IntegerVector: incremental list of labels corresponding to each F2 node, self-prescribed or supervised.\nn_categories::Int: number of category weights (F2 nodes).\nepoch::Int: current training epoch.\n\nReferences\n\nG. A. Carpenter, S. Grossberg, N. Markuzon, J. H. Reynolds, and D. B. Rosen, “Fuzzy ARTMAP: A Neural Network Architecture for Incremental Supervised Learning of Analog Multidimensional Maps,” IEEE Trans. Neural Networks, vol. 3, no. 5, pp. 698-713, 1992, doi: 10.1109/72.159059.\n\n\n\n\n\n","category":"type"},{"location":"man/full-index/#AdaptiveResonance.opts_DDVFA","page":"Index","title":"AdaptiveResonance.opts_DDVFA","text":"opts_DDVFA(;kwargs)\n\nDistributed Dual Vigilance Fuzzy ART options struct.\n\nKeyword Arguments\n\nrho_lb::Float: lower-bound vigilance value, [0, 1], default 0.7.\nrho_ub::Float: upper-bound vigilance value, [0, 1], default 0.85.\nalpha::Float: choice parameter, alpha > 0, default 1e-3.\nbeta::Float: learning parameter, (0, 1], default 1.0.\ngamma::Float: \"pseudo\" kernel width, gamma >= 1, default 3.0.\ngamma_ref::Float: \"reference\" kernel width, 0 <= gamma_ref < gamma, default 1.0.\nmethod::String: similarity method (activation and match):\n\nsingle, average, complete, median, weighted, or centroid, default single.\n\ndisplay::Bool: display flag, default true.\nmax_epoch::Int: maximum number of epochs during training, default 1.\ngamma_normalization::Bool: normalize the threshold by the feature dimension, default true.\n\n\n\n\n\n","category":"type"},{"location":"man/full-index/#AdaptiveResonance.opts_DVFA","page":"Index","title":"AdaptiveResonance.opts_DVFA","text":"opts_DVFA(;kwargs)\n\nDual Vigilance Fuzzy ART options struct.\n\nKeyword Arguments\n\nrho_lb::Float: lower-bound vigilance value, [0, 1], default 0.55.\nrho_ub::Float: upper-bound vigilance value, [0, 1], default 0.75.\nalpha::Float: choice parameter, alpha > 0, default 1e-3.\nbeta::Float: learning parameter, (0, 1], default 1.0.\ndisplay::Bool: display flag, default true.\nmax_epoch::Int: maximum number of epochs during training, default 1.\n\n\n\n\n\n","category":"type"},{"location":"man/full-index/#AdaptiveResonance.opts_FuzzyART","page":"Index","title":"AdaptiveResonance.opts_FuzzyART","text":"opts_FuzzyART(;kwargs)\n\nGamma-Normalized Fuzzy ART options struct.\n\nKeyword Arguments\n\nrho::Float: vigilance value, [0, 1], default 0.6.\nalpha::Float: choice parameter, alpha > 0, default 1e-3.\nbeta::Float: learning parameter, (0, 1], default 1.0.\ngamma::Float: \"pseudo\" kernel width, gamma >= 1, default 3.0.\ngamma_ref::Float: \"reference\" kernel width, 0 <= gamma_ref < gamma, default 1.0.\ndisplay::Bool: display flag, default true.\nmax_epoch::Int: maximum number of epochs during training, default 1.\ngamma_normalization::Bool: normalize the threshold by the feature dimension, default false.\n\n\n\n\n\n","category":"type"},{"location":"man/full-index/#AdaptiveResonance.opts_DAM","page":"Index","title":"AdaptiveResonance.opts_DAM","text":"opts_DAM(;kwargs)\n\nImplements a Default ARTMAP learner's options.\n\nKeyword Arguments\n\nrho::Float: vigilance value, [0, 1], default 0.75.\nalpha::Float: choice parameter, alpha > 0, default 1e-7.\nepsilon::Float: match tracking parameter, (0, 1), default 1e-3\nbeta::Float: learning parameter, (0, 1], default 1.0.\nuncommitted::Bool: uncommitted node flag, default true.\ndisplay::Bool: display flag, default true.\nmax_epoch::Int: maximum number of epochs during training, default 1.\n\n\n\n\n\n","category":"type"},{"location":"man/full-index/#AdaptiveResonance.opts_FAM","page":"Index","title":"AdaptiveResonance.opts_FAM","text":"opts_FAM(;kwargs)\n\nImplements a Fuzzy ARTMAP learner's options.\n\nKeyword Arguments\n\nrho::Float: vigilance value, [0, 1], default 0.6.\nalpha::Float: choice parameter, alpha > 0, default 1e-7.\nepsilon::Float: match tracking parameter, (0, 1), default 1e-3\nbeta::Float: learning parameter, (0, 1], default 1.0.\nuncommitted::Bool: uncommitted node flag, default true.\ndisplay::Bool: display flag, default true.\nmax_epoch::Int: maximum number of epochs during training, default 1.\n\n\n\n\n\n","category":"type"},{"location":"man/full-index/#AdaptiveResonance.opts_SFAM","page":"Index","title":"AdaptiveResonance.opts_SFAM","text":"opts_SFAM(;kwargs)\n\nImplements a Simple Fuzzy ARTMAP learner's options.\n\nKeyword Arguments\n\nrho::Float: vigilance value, [0, 1], default 0.75.\nalpha::Float: choice parameter, alpha > 0, default 1e-7.\nepsilon::Float: match tracking parameter, (0, 1), default 1e-3\nbeta::Float: learning parameter, (0, 1], default 1.0.\nuncommitted::Bool: uncommitted node flag, default true.\ndisplay::Bool: display flag, default true.\nmax_epoch::Int: maximum number of epochs during training, default 1.\n\n\n\n\n\n","category":"type"},{"location":"man/full-index/#AdaptiveResonance.DataConfig","page":"Index","title":"AdaptiveResonance.DataConfig","text":"DataConfig\n\nContainer to standardize training/testing data configuration.\n\nParameters\n\nsetup::Bool: flag if data has been setup yet or not.\nmins::RealVector: list of minimum values for each feature.\nmaxs::RealVector: list of maximum values for each feature.\ndim::Int: dimensionality of the feature vectors (i.e., number of features).\ndim_comp::Int complement coded feature dimensionality, twice the size of dim.\n\n\n\n\n\n","category":"type"},{"location":"man/full-index/#AdaptiveResonance.ARTModule","page":"Index","title":"AdaptiveResonance.ARTModule","text":"ARTModule\n\nAbstract supertype for both ART (unsupervised) and ARTMAP (supervised) modules.\n\n\n\n\n\n","category":"type"},{"location":"man/full-index/#AdaptiveResonance.ART","page":"Index","title":"AdaptiveResonance.ART","text":"ART <: ARTModule\n\nAbstract supertype for all default unsupervised ART modules.\n\n\n\n\n\n","category":"type"},{"location":"man/full-index/#AdaptiveResonance.ARTMAP","page":"Index","title":"AdaptiveResonance.ARTMAP","text":"ARTMAP <: ARTModule\n\nAbstract supertype for all supervised ARTMAP modules.\n\n\n\n\n\n","category":"type"},{"location":"man/full-index/#AdaptiveResonance.ARTOpts","page":"Index","title":"AdaptiveResonance.ARTOpts","text":"ARTOpts\n\nAbstract supertype for all ART module options.\n\n\n\n\n\n","category":"type"},{"location":"examples/adaptive_resonance/incremental-batch/#incremental_batch","page":"Incremental vs. Batch Example","title":"Incremental vs. Batch Example","text":"","category":"section"},{"location":"examples/adaptive_resonance/incremental-batch/","page":"Incremental vs. Batch Example","title":"Incremental vs. Batch Example","text":"(Image: Source code) (Image: notebook) (Image: compat) (Image: Author) (Image: Update time)","category":"page"},{"location":"examples/adaptive_resonance/incremental-batch/#Overview","page":"Incremental vs. Batch Example","title":"Overview","text":"","category":"section"},{"location":"examples/adaptive_resonance/incremental-batch/","page":"Incremental vs. Batch Example","title":"Incremental vs. Batch Example","text":"All modules in AdaptiveResonance.jl are designed to handle incremental and batch training. In fact, ART modules are generally incremental in their implementation, so their batch methods wrap the incremental ones and handle preprocessing, etc. For example, DDVFA can be run incrementally (i.e. with one sample at a time) with custom algorithmic options and a predetermined data configuration.","category":"page"},{"location":"examples/adaptive_resonance/incremental-batch/","page":"Incremental vs. Batch Example","title":"Incremental vs. Batch Example","text":"note: Note\nIn the incremental case, it is necessary to provide a data configuration if the model is not pretrained because the model has no knowledge of the boundaries and dimensionality of the data, which are necessary in the complement coding step. For more info, see the guide in the docs on incremental vs. batch.","category":"page"},{"location":"examples/adaptive_resonance/incremental-batch/#Data-Setup","page":"Incremental vs. Batch Example","title":"Data Setup","text":"","category":"section"},{"location":"examples/adaptive_resonance/incremental-batch/","page":"Incremental vs. Batch Example","title":"Incremental vs. Batch Example","text":"We begin with importing AdaptiveResonance for the ART modules and MLDatasets for some data utilities.","category":"page"},{"location":"examples/adaptive_resonance/incremental-batch/","page":"Incremental vs. Batch Example","title":"Incremental vs. Batch Example","text":"using AdaptiveResonance # ART\nusing MLDatasets        # Iris dataset\nusing MLDataUtils       # Shuffling and splitting\nusing Printf            # Formatted number printing","category":"page"},{"location":"examples/adaptive_resonance/incremental-batch/","page":"Incremental vs. Batch Example","title":"Incremental vs. Batch Example","text":"We will download the Iris dataset for its small size and benchmark use for clustering algorithms.","category":"page"},{"location":"examples/adaptive_resonance/incremental-batch/","page":"Incremental vs. Batch Example","title":"Incremental vs. Batch Example","text":"Iris.download(i_accept_the_terms_of_use=true)\nfeatures, labels = Iris.features(), Iris.labels()","category":"page"},{"location":"examples/adaptive_resonance/incremental-batch/","page":"Incremental vs. Batch Example","title":"Incremental vs. Batch Example","text":"Because the MLDatasets package gives us Iris labels as strings, we will use the MLDataUtils.convertlabel method with the MLLabelUtils.LabelEnc.Indices type to get a list of integers representing each class:","category":"page"},{"location":"examples/adaptive_resonance/incremental-batch/","page":"Incremental vs. Batch Example","title":"Incremental vs. Batch Example","text":"labels = convertlabel(LabelEnc.Indices{Int}, labels)\nunique(labels)","category":"page"},{"location":"examples/adaptive_resonance/incremental-batch/","page":"Incremental vs. Batch Example","title":"Incremental vs. Batch Example","text":"Next, we will create a train/test split with the MLDataUtils.stratifiedobs utility:","category":"page"},{"location":"examples/adaptive_resonance/incremental-batch/","page":"Incremental vs. Batch Example","title":"Incremental vs. Batch Example","text":"(X_train, y_train), (X_test, y_test) = stratifiedobs((features, labels))","category":"page"},{"location":"examples/adaptive_resonance/incremental-batch/#Incremental-vs.-Batch","page":"Incremental vs. Batch Example","title":"Incremental vs. Batch","text":"","category":"section"},{"location":"examples/adaptive_resonance/incremental-batch/#Setup","page":"Incremental vs. Batch Example","title":"Setup","text":"","category":"section"},{"location":"examples/adaptive_resonance/incremental-batch/","page":"Incremental vs. Batch Example","title":"Incremental vs. Batch Example","text":"Now, we can create several modules to illustrate training one in batch and one incrementaly.","category":"page"},{"location":"examples/adaptive_resonance/incremental-batch/","page":"Incremental vs. Batch Example","title":"Incremental vs. Batch Example","text":"# Create several modules for batch and incremental training.\n# We can take advantage of the options instantiation method here to use the same options for both modules.\nopts = opts_DDVFA(rho_lb=0.6, rho_ub=0.75)\nart_batch = DDVFA(opts)\nart_incremental = DDVFA(opts)","category":"page"},{"location":"examples/adaptive_resonance/incremental-batch/","page":"Incremental vs. Batch Example","title":"Incremental vs. Batch Example","text":"For the incremental version, we must setup the data configuration in advance. In batch mode, this is done automatically based upon the provided data, but the incremental variant has not way of knowing the bounds of the individual features. We could preprocess the data and set the data configuration with art.config = DataConfig(0, 1, 4), which translates to the data containing four features  that all range from 0 to 1. This would be done in scenarios where we have either done some preprocessing on the data or have prior knowledge about the bounds of individual features. However, in this example we will let the module determine the bounds with the convenience method data_setup!:","category":"page"},{"location":"examples/adaptive_resonance/incremental-batch/","page":"Incremental vs. Batch Example","title":"Incremental vs. Batch Example","text":"# Setup the data config on all of the features.\ndata_setup!(art_incremental.config, features)","category":"page"},{"location":"examples/adaptive_resonance/incremental-batch/#Training","page":"Incremental vs. Batch Example","title":"Training","text":"","category":"section"},{"location":"examples/adaptive_resonance/incremental-batch/","page":"Incremental vs. Batch Example","title":"Incremental vs. Batch Example","text":"We can train in batch with a simple supervised mode by passing the labels as a keyword argument.","category":"page"},{"location":"examples/adaptive_resonance/incremental-batch/","page":"Incremental vs. Batch Example","title":"Incremental vs. Batch Example","text":"y_hat_batch_train = train!(art_batch, X_train, y=y_train)\nprintln(\"Training labels: \",  size(y_hat_batch_train), \" \", typeof(y_hat_batch_train))","category":"page"},{"location":"examples/adaptive_resonance/incremental-batch/","page":"Incremental vs. Batch Example","title":"Incremental vs. Batch Example","text":"We can also train incrementally with the same method, being careful that we pass a vector features and a single integer as the labels","category":"page"},{"location":"examples/adaptive_resonance/incremental-batch/","page":"Incremental vs. Batch Example","title":"Incremental vs. Batch Example","text":"# Get the number of training samples\nn_train = length(y_train)\n# Create a container for the training output labels\ny_hat_incremental_train = zeros(Int, n_train)\n# Iterate over all training samples\nfor ix = 1:length(y_train)\n    sample = X_train[:, ix]\n    label = y_train[ix]\n    y_hat_incremental_train[ix] = train!(art_incremental, sample, y=label)\nend","category":"page"},{"location":"examples/adaptive_resonance/incremental-batch/#Testing","page":"Incremental vs. Batch Example","title":"Testing","text":"","category":"section"},{"location":"examples/adaptive_resonance/incremental-batch/","page":"Incremental vs. Batch Example","title":"Incremental vs. Batch Example","text":"We can then classify both networks and check that their performances are equivalent. For both, we will use the best-matching unit in the case of complete mismatch (see the docs on Mismatch vs. BMU)","category":"page"},{"location":"examples/adaptive_resonance/incremental-batch/","page":"Incremental vs. Batch Example","title":"Incremental vs. Batch Example","text":"# Classify one model in batch mode\ny_hat_batch = AdaptiveResonance.classify(art_batch, X_test, get_bmu=true)\n\n# Classify one model incrementally\nn_test = length(y_test)\ny_hat_incremental = zeros(Int, n_test)\nfor ix = 1:n_test\n    y_hat_incremental[ix] = AdaptiveResonance.classify(art_incremental, X_test[:, ix], get_bmu=true)\nend\n\n# Check the shape and type of the output labels\nprintln(\"Batch testing labels: \",  size(y_hat_batch), \" \", typeof(y_hat_batch))\nprintln(\"Incremental testing labels: \",  size(y_hat_incremental), \" \", typeof(y_hat_incremental))","category":"page"},{"location":"examples/adaptive_resonance/incremental-batch/","page":"Incremental vs. Batch Example","title":"Incremental vs. Batch Example","text":"Finally, we check the performance (number of correct classifications over total number of test samples) for both models, verifying that they produce the same results.","category":"page"},{"location":"examples/adaptive_resonance/incremental-batch/","page":"Incremental vs. Batch Example","title":"Incremental vs. Batch Example","text":"# Calculate performance on training data, testing data, and with get_bmu\nperf_train_batch = performance(y_hat_batch_train, y_train)\nperf_train_incremental = performance(y_hat_incremental_train, y_train)\nperf_test_batch = performance(y_hat_batch, y_test)\nperf_test_incremental = performance(y_hat_incremental, y_test)\n\n# Format each performance number for comparison\n@printf \"Batch training performance: %.4f\\n\" perf_train_batch\n@printf \"Incremental training performance: %.4f\\n\" perf_train_incremental\n@printf \"Batch testing performance: %.4f\\n\" perf_test_batch\n@printf \"Incremental testing performance: %.4f\\n\" perf_test_incremental","category":"page"},{"location":"examples/adaptive_resonance/incremental-batch/#Visualization","page":"Incremental vs. Batch Example","title":"Visualization","text":"","category":"section"},{"location":"examples/adaptive_resonance/incremental-batch/","page":"Incremental vs. Batch Example","title":"Incremental vs. Batch Example","text":"So we showed that the performance and behavior of modules are identical in incremental and batch modes. Great! However, illustrating this point doesn't lend itself to visualization in any meaningful way. Nonetheless, we would like a pretty picture at the end of the experiment to verify that these identical solutions work in the first place.","category":"page"},{"location":"examples/adaptive_resonance/incremental-batch/","page":"Incremental vs. Batch Example","title":"Incremental vs. Batch Example","text":"To do this, we will reduce the dimensionality of the dataset to two dimensions and show in a scatter plot how the modules classify the test data into groups. This will be done with principal component analysis (PCA) to cast the points into a 2-D space while trying to preserve the relative distances between points in the higher dimension. The process isn't perfect by any means, but it suffices for visualization.","category":"page"},{"location":"examples/adaptive_resonance/incremental-batch/","page":"Incremental vs. Batch Example","title":"Incremental vs. Batch Example","text":"# Import visualization utilities\nusing Printf            # Formatted number printing\nusing MultivariateStats # Principal component analysis (PCA)\nusing Plots             # Plotting frontend\npyplot()                # Use PyPlot backend\n\n# Train a PCA model\nM = fit(PCA, features; maxoutdim=2)\n\n# Apply the PCA model to the testing set\nX_test_pca = transform(M, X_test)","category":"page"},{"location":"examples/adaptive_resonance/incremental-batch/","page":"Incremental vs. Batch Example","title":"Incremental vs. Batch Example","text":"Now that we have the test points cast into a 2-D set of points, we can create a scatter plot that shows how each point is categorized by the modules.","category":"page"},{"location":"examples/adaptive_resonance/incremental-batch/","page":"Incremental vs. Batch Example","title":"Incremental vs. Batch Example","text":"# Create a scatterplot object from the data\np1 = scatter(\n    X_test_pca[1, :],       # PCA dimension 1\n    X_test_pca[2, :],       # PCA dimension 2\n    group = y_hat_batch,    # labels belonging to each point\n    markersize = 8,         # size of scatter points\n    title = @sprintf \"DDVFA Iris Clusters\"    # formatted title\n)\n\n# Plot the scatterplot with some additonal formatting options\nplot(\n    p1,                     # the scatterplot object\n    legend = false,         # no legend\n    xtickfontsize = 12,     # x-tick size\n    ytickfontsize = 12,     # y-tick size\n    dpi = 300,              # Set the dots-per-inch\n    xlims = :round,         # Round up the x-limits to the nearest whole number\n    xlabel = \"\\$PCA_1\\$\",   # x-label\n    ylabel = \"\\$PCA_2\\$\",   # y-label\n)","category":"page"},{"location":"examples/adaptive_resonance/incremental-batch/","page":"Incremental vs. Batch Example","title":"Incremental vs. Batch Example","text":"This plot shows that the DDVFA modules do well at identifying the structure of the three clusters despite not achieving 100% test performance.","category":"page"},{"location":"examples/adaptive_resonance/incremental-batch/","page":"Incremental vs. Batch Example","title":"Incremental vs. Batch Example","text":"png(\"assets/incremental-batch-cover\") #hide","category":"page"},{"location":"examples/adaptive_resonance/incremental-batch/","page":"Incremental vs. Batch Example","title":"Incremental vs. Batch Example","text":"","category":"page"},{"location":"examples/adaptive_resonance/incremental-batch/","page":"Incremental vs. Batch Example","title":"Incremental vs. Batch Example","text":"This page was generated using DemoCards.jl and Literate.jl.","category":"page"},{"location":"","page":"Home","title":"Home","text":"(Image: header)","category":"page"},{"location":"","page":"Home","title":"Home","text":"","category":"page"},{"location":"#AdaptiveResonance.jl","page":"Home","title":"AdaptiveResonance.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"These pages serve as the official documentation for the AdaptiveResonance.jl Julia package.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Adaptive Resonance Theory (ART) began as a neurocognitive theory of how fields of cells can continuously learn stable representations, and it evolved into the basis for a myriad of practical machine learning algorithms. Pioneered by Stephen Grossberg and Gail Carpenter, the field has had contributions across many years and from many disciplines, resulting in a plethora of engineering applications and theoretical advancements that have enabled ART-based algorithms to compete with many other modern learning and clustering algorithms.","category":"page"},{"location":"","page":"Home","title":"Home","text":"The purpose of this package is to provide a home for the development and use of these ART-based machine learning algorithms.","category":"page"},{"location":"","page":"Home","title":"Home","text":"See the Index for the complete list of documented functions and types.","category":"page"},{"location":"#Manual-Outline","page":"Home","title":"Manual Outline","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"This documentation is split into the following sections:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Pages = [\n    \"man/guide.md\",\n    \"../examples/index.md\",\n    \"man/modules.md\",\n    \"man/contributing.md\",\n    \"man/full-index.md\",\n]\nDepth = 1","category":"page"},{"location":"","page":"Home","title":"Home","text":"The Package Guide provides a tutorial to the full usage of the package, while Examples gives sample workflows using a variety of ART modules. A list of the implemented ART modules is included in Modules, where different options are also listed for creating variants of these modules that exist in the literature.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Instructions on how to contribute to the package are found in Contributing, and docstrings for every element of the package is listed in the Index.","category":"page"}]
}
