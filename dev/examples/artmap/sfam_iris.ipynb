{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "SFAM is a supervised algorithm by definition, so we use it to map a set of features to a set of supervisory labels.\n",
    "We will do so by training and testing on the ubiquitous Iris dataset and seeing how well the SFAM module generalizes the data."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We begin with importing AdaptiveResonance for the ART modules and MLDatasets for some data utilities."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using AdaptiveResonance # ART\n",
    "using MLDatasets        # Iris dataset\n",
    "using MLDataUtils       # Shuffling and splitting\n",
    "using Printf            # Formatted number printing"
   ],
   "metadata": {},
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will download the Iris dataset for its small size and benchmark use for clustering algorithms."
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "([5.1 4.9 … 6.2 5.9; 3.5 3.0 … 3.4 3.0; 1.4 1.4 … 5.4 5.1; 0.2 0.2 … 2.3 1.8], [\"Iris-setosa\", \"Iris-setosa\", \"Iris-setosa\", \"Iris-setosa\", \"Iris-setosa\", \"Iris-setosa\", \"Iris-setosa\", \"Iris-setosa\", \"Iris-setosa\", \"Iris-setosa\"  …  \"Iris-virginica\", \"Iris-virginica\", \"Iris-virginica\", \"Iris-virginica\", \"Iris-virginica\", \"Iris-virginica\", \"Iris-virginica\", \"Iris-virginica\", \"Iris-virginica\", \"Iris-virginica\"])"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "cell_type": "code",
   "source": [
    "# Get the iris dataset as a DataFrame\n",
    "iris = Iris()\n",
    "# Manipulate the features and labels into a matrix of features and a vector of labels\n",
    "features, labels = Matrix(iris.features)', vec(Matrix{String}(iris.targets))"
   ],
   "metadata": {},
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "Because the MLDatasets package gives us Iris labels as strings, we will use the `MLDataUtils.convertlabel` method with the `MLLabelUtils.LabelEnc.Indices` type to get a list of integers representing each class:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "3-element Vector{Int64}:\n 1\n 2\n 3"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "cell_type": "code",
   "source": [
    "labels = convertlabel(LabelEnc.Indices{Int}, labels)\n",
    "unique(labels)"
   ],
   "metadata": {},
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, we will create a train/test split with the `MLDataUtils.stratifiedobs` utility:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(([6.2 5.0 … 5.6 4.7; 3.4 3.5 … 2.8 3.2; 5.4 1.3 … 4.9 1.3; 2.3 0.3 … 2.0 0.2], [3, 1, 2, 1, 3, 2, 3, 3, 1, 3  …  3, 2, 2, 1, 3, 3, 3, 1, 3, 1]), ([5.0 5.7 … 6.3 6.3; 2.0 2.6 … 3.4 2.7; 3.5 3.5 … 5.6 4.9; 1.0 1.0 … 2.4 1.8], [2, 2, 2, 3, 2, 1, 1, 2, 1, 2  …  3, 1, 2, 2, 2, 2, 1, 3, 3, 3]))"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "cell_type": "code",
   "source": [
    "(X_train, y_train), (X_test, y_test) = stratifiedobs((features, labels))"
   ],
   "metadata": {},
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, we can create our SFAM module.\n",
    "We'll do so with the default contstructor, though the module itself has many options that can be altered during instantiation."
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.01"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "cell_type": "code",
   "source": [
    "# Create the SFAM module\n",
    "art = SFAM()\n",
    "\n",
    "# Change the match tracking parameter after instantiation\n",
    "art.opts.epsilon = 1e-2"
   ],
   "metadata": {},
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can train the model in batch mode upon the data and supervisory labels.\n",
    "We do so by directly passing the integer vector of labels to the training method.\n",
    "Just as in other modules, we can extract the SFAM's prescribed labels from the training method, which should match up to the training labels as we will see later."
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training labels: (105,) Vector{Int64}\n"
     ]
    }
   ],
   "cell_type": "code",
   "source": [
    "# Train in supervised mode by directly passing the labels.\n",
    "y_hat_train = train!(art, X_train, y_train)\n",
    "println(\"Training labels: \",  size(y_hat_train), \" \", typeof(y_hat_train))"
   ],
   "metadata": {},
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can classify the testing data to see how we generalize.\n",
    "At the same time, we can see the effect of getting the best-matching unit in the case of complete mismatch (see the docs on Mismatch vs. BMU)"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing labels: (45,) Vector{Int64}\n",
      "Testing labels with bmu: (45,) Vector{Int64}\n"
     ]
    }
   ],
   "cell_type": "code",
   "source": [
    "# Classify both ways\n",
    "y_hat = AdaptiveResonance.classify(art, X_test)\n",
    "y_hat_bmu = AdaptiveResonance.classify(art, X_test, get_bmu=true)\n",
    "\n",
    "# Check the shape and type of the output labels\n",
    "println(\"Testing labels: \",  size(y_hat), \" \", typeof(y_hat))\n",
    "println(\"Testing labels with bmu: \",  size(y_hat_bmu), \" \", typeof(y_hat_bmu))"
   ],
   "metadata": {},
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally, we can calculate the performances (number correct over total) of the model upon all three regimes:\n",
    "1. Training data\n",
    "2. Testing data\n",
    "2. Testing data with `get_bmu=true`"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training performance: 1.0000\n",
      "Testing performance: 0.8667\n",
      "Best-matching unit testing performance: 0.8667\n"
     ]
    }
   ],
   "cell_type": "code",
   "source": [
    "# Calculate performance on training data, testing data, and with get_bmu\n",
    "perf_train = performance(y_hat_train, y_train)\n",
    "perf_test = performance(y_hat, y_test)\n",
    "perf_test_bmu = performance(y_hat_bmu, y_test)\n",
    "\n",
    "# Format each performance number for comparison\n",
    "@printf \"Training performance: %.4f\\n\" perf_train\n",
    "@printf \"Testing performance: %.4f\\n\" perf_test\n",
    "@printf \"Best-matching unit testing performance: %.4f\\n\" perf_test_bmu"
   ],
   "metadata": {},
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ],
   "metadata": {}
  }
 ],
 "nbformat_minor": 3,
 "metadata": {
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.3"
  },
  "kernelspec": {
   "name": "julia-1.8",
   "display_name": "Julia 1.8.3",
   "language": "julia"
  }
 },
 "nbformat": 4
}
